{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import pickle\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "data = pandas.read_pickle('raw_processed.pkl')\n",
    "\n",
    "# make a short version for better viewing\n",
    "dataShort = data.iloc[0:10,19:46].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c6b61cf828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.closest_pos.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\pandas\\core\\frame.py:3930: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235.0\n"
     ]
    }
   ],
   "source": [
    "# set the number of adjacent soundings to use\n",
    "adSoundings=1;\n",
    "\n",
    "# get the line numbers, find the time difference between adjacent rows, and set a threshold for two points to be considered \n",
    "# part of the same sounding\n",
    "lineNumbers=np.unique(data[\"LINE_NO\"])\n",
    "timeDiff=np.diff(data[\"TIMESTAMP\"])\n",
    "timeDiffMask=timeDiff<5e-6\n",
    "\n",
    "# iterate through the line numbers\n",
    "for line in lineNumbers:\n",
    "    \n",
    "# this is for debugging\n",
    "#if 1==1:\n",
    "#    line=lineNumbers[0]\n",
    "\n",
    "    # make a mask for the rows in the big DF that are for this line number\n",
    "    rowIndex=data[\"LINE_NO\"]==line\n",
    "    \n",
    "    # get just the current rows\n",
    "    currData=data.loc[rowIndex,:]\n",
    "    currData.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # make an array of the time differences\n",
    "    timeDiff=np.diff(currData[\"TIMESTAMP\"])\n",
    "\n",
    "    # if the first time difference is large, drop the first row, this throws a warning\n",
    "    if timeDiff[0]>1e-5:\n",
    "        currData.drop([0], inplace=True)\n",
    "        currData.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # set the number of pairs in this line and make some arrays with large numbers for the HM and LM data\n",
    "    numberPairs=np.int(len(currData.index)/2)\n",
    "    currDataLM=np.ones((numberPairs,1+adSoundings*2,37))*9000\n",
    "    currDataHM=np.ones((numberPairs,1+adSoundings*2,37))*9000\n",
    "    currLabelsLM=np.zeros((numberPairs,1))\n",
    "    currLabelsHM=np.zeros((numberPairs,1))\n",
    "    \n",
    "    # check to see if the first row is LM or HM, set the indexing\n",
    "    if np.mean(currData.loc[0,'DBDT_Ch1GT1':'DBDT_Ch1GT28'])>9990:\n",
    "        hmFirst=1\n",
    "    else:\n",
    "        hmFirst=0\n",
    "               \n",
    "    # iterate through the number of pairs\n",
    "    for i in range(numberPairs):\n",
    "        if hmFirst==1:\n",
    "            if adSoundings==1:\n",
    "                hmIndex=[(i-1)*2,i*2,(i+1)*2]\n",
    "                lmIndex=[(i-1)*2+1,i*2+1,(i+1)*2+1]\n",
    "            elif adSoundings==2:\n",
    "                hmIndex=[(i-2)*2,(i-1)*2,i*2,(i+1)*2,(i+2)*2]\n",
    "                lmIndex=[(i-2)*2+1,(i-1)*2+1,i*2+1,(i+1)*2+1,(i+2)*2+1]\n",
    "        else:\n",
    "            if adSoundings==1:\n",
    "                lmIndex=[(i-1)*2,i*2,(i+1)*2]\n",
    "                hmIndex=[(i-1)*2+1,i*2+1,(i+1)*2+1]\n",
    "            elif adSoundings==2:\n",
    "                hmIndex=[(i-2)*2,(i-1)*2,i*2,(i+1)*2,(i+2)*2]\n",
    "                lmIndex=[(i-2)*2+1,(i-1)*2+1,i*2+1,(i+1)*2+1,(i+2)*2+1]\n",
    "        \n",
    "        # for the first pair, leave the sounding in position 0 alone as it doesnt exist, do the same for the last pair but \n",
    "        # for the sounding in position 2\n",
    "        # TODO: modify this to work for 2 or 3 or 4 or 5 adjacent soundings\n",
    "        if i==0:\n",
    "            currDataLM[i,2,0:28]=currData.loc[lmIndex[2],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataHM[i,2,:]=currData.loc[hmIndex[2],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "        elif i==numberPairs-1:\n",
    "            currDataLM[i,0,0:28]=currData.loc[lmIndex[0],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataHM[i,0,:]=currData.loc[hmIndex[0],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "            \n",
    "        # for every other pair fill in both adjacent soundings\n",
    "        # TODO: modify this to work for 2 or 3 or 4 or 5 adjacent soundings\n",
    "        else:\n",
    "            currDataLM[i,0,0:28]=currData.loc[lmIndex[0],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataLM[i,2,0:28]=currData.loc[lmIndex[2],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataHM[i,0,:]=currData.loc[hmIndex[0],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "            currDataHM[i,2,:]=currData.loc[hmIndex[2],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "            \n",
    "        # middle sounding and labels always get set for every pair\n",
    "        currDataLM[i,1,0:28]=currData.loc[lmIndex[1],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "        currDataHM[i,1,:]=currData.loc[hmIndex[1],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "        currLabelsLM[i,0]=currData.loc[i*2+1,'VALID']\n",
    "        currLabelsHM[i,0]=currData.loc[i*2,'VALID']\n",
    "    \n",
    "    # set large values to mean of other values\n",
    "    for k in range(0,37):\n",
    "        \n",
    "        # get the current timegate\n",
    "        tempDataHM=currDataHM[:,:,k]\n",
    "        \n",
    "        # set values over 5000 to the mean of the timegate\n",
    "        tempDataHM[np.abs(tempDataHM)>5000]=np.mean(tempDataHM[np.abs(tempDataHM)<5000])\n",
    "        \n",
    "        # do the same to nans\n",
    "        tempDataHM=np.nan_to_num(tempDataHM, nan=np.mean(tempDataHM[np.abs(tempDataHM)<5000]))\n",
    "        \n",
    "        # check to see if it fucking worked\n",
    "        if np.isnan(tempDataHM).any():\n",
    "            print(\"wtf\")\n",
    "        currDataHM[:,:,k]=tempDataHM\n",
    "        \n",
    "        # do the same thing but for the LM\n",
    "        tempDataLM=currDataLM[:,:,k]\n",
    "        tempDataLM[np.abs(tempDataLM)>5000]=np.mean(tempDataLM[np.abs(tempDataLM)<5000])\n",
    "        \n",
    "        # doesnt work, will deal with the nans below\n",
    "        tempDataLM=np.nan_to_num(tempDataLM, nan=np.mean(tempDataLM[np.abs(tempDataLM)<5000]))\n",
    "        #if np.isnan(tempDataLM).any():\n",
    "        #    print(\"wtf LM\",np.mean(tempDataLM[np.abs(tempDataLM)<5000]))\n",
    "        currDataLM[:,:,k]=tempDataLM\n",
    "\n",
    "    # now build one array with everything, if its the first line make it, otherwise append\n",
    "    if line==lineNumbers[0]:\n",
    "        dataLM=currDataLM\n",
    "        dataHM=currDataHM\n",
    "        labelsLM=currLabelsLM\n",
    "        labelsHM=currLabelsHM\n",
    "    else:\n",
    "        dataLM=np.append(dataLM,currDataLM, axis=0)\n",
    "        dataHM=np.append(dataHM,currDataHM, axis=0)\n",
    "        labelsLM=np.append(labelsLM,currLabelsLM, axis=0)\n",
    "        labelsHM=np.append(labelsHM,currLabelsHM, axis=0)\n",
    "\n",
    "# drop rows where the lm and hm labels dont agree\n",
    "indexGood=labelsHM==labelsLM\n",
    "\n",
    "# see how many locations this happened in\n",
    "print(np.sum(np.abs(labelsLM-labelsHM)))\n",
    "\n",
    "# keep only the rows where they agree\n",
    "dataLM=dataLM[indexGood[:,0],:,:]\n",
    "dataHM=dataHM[indexGood[:,0],:,:]\n",
    "labelsLM=labelsLM[indexGood[:,0],:]\n",
    "labelsHM=labelsHM[indexGood[:,0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 20000\n",
      "number of test examples = 3760\n",
      "X_train shape: (20000, 3, 37, 2)\n",
      "Y_train shape: (20000, 1)\n",
      "X_test shape: (3760, 3, 37, 2)\n",
      "Y_test shape: (3760, 1)\n"
     ]
    }
   ],
   "source": [
    "# make X from the HM and LM data, X is m (examples) by 3 (first adjacent sounding, middle sounding, other adjacent sounding) \n",
    "# by 37 (timegates) by 2 (low moment then high moment)\n",
    "X=np.zeros([np.shape(dataLM)[0],np.shape(dataLM)[1],np.shape(dataLM)[2],2])\n",
    "X[:,:,:,0]=dataLM\n",
    "X[:,:,:,1]=dataHM\n",
    "\n",
    "# set nans to 0 (there shouldnt be any but who knows)\n",
    "X=np.nan_to_num(X)\n",
    "\n",
    "# scale each timegate to be between -1 and 1\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "for i in range(np.shape(X)[-1]):\n",
    "    for j in range(np.shape(X)[1]):\n",
    "        \n",
    "        # do it for all examples, for this moment\n",
    "        X[:,j,:,i] = min_max_scaler.fit_transform(X[:,j,:,i])\n",
    "        \n",
    "        # for the timegates where the LM never exists, set it back to 0 so it sits in the middle of the scaled range\n",
    "        for k in range(np.shape(X)[2]):\n",
    "            if i==0:\n",
    "                if k>=28:\n",
    "                    X[:,j,k,i]=X[:,j,k,i]-X[:,j,k,i]\n",
    "\n",
    "# make some random indices, then take 20000 examples for training and the rest for test, results in about 85/15 split\n",
    "indices=np.random.permutation(X.shape[0])\n",
    "X_train=X[indices[:20000],:,:,:]\n",
    "X_test=X[indices[20000:],:,:,:]\n",
    "Y_train=labelsLM[indices[:20000],:]\n",
    "Y_test=labelsLM[indices[20000:],:]\n",
    "\n",
    "# print out some shit\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  11.,   13.,   78.,  247.,  636., 1778., 5079., 8852., 5987.,\n",
       "        1079.]),\n",
       " array([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at a timegate to check the scaling\n",
    "plt.hist(X[:,1,3,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "\n",
    "    \"\"\"\n",
    "    input_shape: The height, width and channels as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    #X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # initializer to use\n",
    "    initToUse=keras.initializers.glorot_normal(seed=0)\n",
    "    #initToUse=keras.initializers.he_normal(seed=0)\n",
    "    \n",
    "    # for the leakly relu\n",
    "    alphaParam=0.3\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(16, (2, 3), strides = (1, 1), kernel_initializer=initToUse, name = 'conv0', padding='same')(X_input)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool0')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(32, (2, 5), strides = (1, 1), kernel_initializer=initToUse, name = 'conv1', padding='same')(X)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool1')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(64, (2, 7), strides = (1, 1), kernel_initializer=initToUse, name = 'conv2', padding='same')(X)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool2')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(128, (2, 9), strides = (1, 1), kernel_initializer=initToUse, name = 'conv3', padding='same')(X)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(128, (2, 11), strides = (1, 1), kernel_initializer=initToUse, name = 'conv4', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool_final')(X)\n",
    "    \n",
    "    # make another initializer\n",
    "    initToUse2=keras.initializers.he_normal(seed=0)\n",
    "    \n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(200, activation='relu', kernel_initializer=initToUse2,name='fc0')(X)\n",
    "    X = Dense(100, activation='relu', kernel_initializer=initToUse2,name='fc1')(X)\n",
    "    X = Dense(50, activation='relu', kernel_initializer=initToUse2,name='fc2')(X)\n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer=initToUse,name='fc3')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelv2(inputShape):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # make the initializer\n",
    "    initToUse=keras.initializers.glorot_normal(seed=0)\n",
    "    initToUse2=keras.initializers.he_normal(seed=0)\n",
    "\n",
    "    # for the leakly relu\n",
    "    alphaParam=0.3\n",
    "    \n",
    "    # short dim\n",
    "    shortDim=3\n",
    "    \n",
    "    # add some conv layers\n",
    "    model.add(Conv2D(16, (shortDim, 3), input_shape=inputShape, kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    model.add(Conv2D(32, (shortDim, 5), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    model.add(Conv2D(64, (shortDim, 7), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (shortDim, 9), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    model.add(Conv2D(256, (shortDim, 11), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    # flatten and do some dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dense(160, activation='relu'))\n",
    "    model.add(Dense(96, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    #X = Conv2D(16, (2, 3), strides = (1, 1), kernel_initializer=initToUse, name = 'conv0', padding='same')(X_input)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool0')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(32, (2, 5), strides = (1, 1), kernel_initializer=initToUse, name = 'conv1', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool1')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(64, (2, 7), strides = (1, 1), kernel_initializer=initToUse, name = 'conv2', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool2')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(128, (2, 9), strides = (1, 1), kernel_initializer=initToUse, name = 'conv3', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(128, (2, 11), strides = (1, 1), kernel_initializer=initToUse, name = 'conv4', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    # input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "    # this applies 32 convolution filters of size 3x3 each.\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(256, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1208: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/300\n",
      "20000/20000 [==============================] - 5s - loss: 0.6023 - acc: 0.7131     \n",
      "Epoch 2/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5642 - acc: 0.7328     \n",
      "Epoch 3/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5541 - acc: 0.7480     \n",
      "Epoch 4/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5423 - acc: 0.7569     \n",
      "Epoch 5/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5400 - acc: 0.7568     \n",
      "Epoch 6/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5318 - acc: 0.7594     \n",
      "Epoch 7/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5390 - acc: 0.7541     \n",
      "Epoch 8/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5240 - acc: 0.7654     \n",
      "Epoch 9/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5040 - acc: 0.7791     \n",
      "Epoch 10/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5038 - acc: 0.7809     \n",
      "Epoch 11/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5018 - acc: 0.7775     \n",
      "Epoch 12/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4961 - acc: 0.7834     \n",
      "Epoch 13/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4907 - acc: 0.7869     \n",
      "Epoch 14/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4929 - acc: 0.7844     \n",
      "Epoch 15/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4800 - acc: 0.7955     \n",
      "Epoch 16/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4794 - acc: 0.7948     - ETA: 0s - loss: 0.4690 - acc: 0.\n",
      "Epoch 17/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4917 - acc: 0.7874     \n",
      "Epoch 18/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4745 - acc: 0.7977     \n",
      "Epoch 19/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4748 - acc: 0.7946     \n",
      "Epoch 20/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4854 - acc: 0.7892     \n",
      "Epoch 21/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4745 - acc: 0.7944     \n",
      "Epoch 22/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4684 - acc: 0.8002     \n",
      "Epoch 23/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4598 - acc: 0.8021     \n",
      "Epoch 24/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4736 - acc: 0.7933     \n",
      "Epoch 25/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4709 - acc: 0.7924     \n",
      "Epoch 26/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4551 - acc: 0.8084     \n",
      "Epoch 27/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4528 - acc: 0.8057     \n",
      "Epoch 28/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4418 - acc: 0.8129     \n",
      "Epoch 29/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4345 - acc: 0.8160     \n",
      "Epoch 30/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4270 - acc: 0.8213     \n",
      "Epoch 31/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4413 - acc: 0.8121     \n",
      "Epoch 32/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4236 - acc: 0.8212     \n",
      "Epoch 33/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4303 - acc: 0.8182     \n",
      "Epoch 34/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4524 - acc: 0.8068     \n",
      "Epoch 35/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4319 - acc: 0.8184     \n",
      "Epoch 36/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4333 - acc: 0.8173     \n",
      "Epoch 37/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4226 - acc: 0.8204     \n",
      "Epoch 38/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4102 - acc: 0.8271     \n",
      "Epoch 39/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4214 - acc: 0.8210     \n",
      "Epoch 40/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3977 - acc: 0.8319     \n",
      "Epoch 41/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4010 - acc: 0.8327     \n",
      "Epoch 42/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4073 - acc: 0.8281     \n",
      "Epoch 43/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4091 - acc: 0.8280     \n",
      "Epoch 44/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3870 - acc: 0.8405     \n",
      "Epoch 45/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3856 - acc: 0.8375     \n",
      "Epoch 46/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3868 - acc: 0.8375     \n",
      "Epoch 47/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3871 - acc: 0.8363     \n",
      "Epoch 48/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3964 - acc: 0.8344     \n",
      "Epoch 49/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3835 - acc: 0.8399     \n",
      "Epoch 50/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3812 - acc: 0.8394     \n",
      "Epoch 51/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3821 - acc: 0.8379     \n",
      "Epoch 52/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3949 - acc: 0.8340     \n",
      "Epoch 53/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3666 - acc: 0.8481     \n",
      "Epoch 54/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3687 - acc: 0.8453     \n",
      "Epoch 55/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3799 - acc: 0.8395     \n",
      "Epoch 56/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3586 - acc: 0.8503     \n",
      "Epoch 57/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3612 - acc: 0.8492     \n",
      "Epoch 58/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3620 - acc: 0.8490     \n",
      "Epoch 59/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3637 - acc: 0.8498     \n",
      "Epoch 60/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3488 - acc: 0.8555     \n",
      "Epoch 61/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3628 - acc: 0.8495     \n",
      "Epoch 62/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3492 - acc: 0.8557     \n",
      "Epoch 63/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3496 - acc: 0.8541     \n",
      "Epoch 64/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3498 - acc: 0.8534     \n",
      "Epoch 65/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3399 - acc: 0.8589     \n",
      "Epoch 66/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3445 - acc: 0.8562     \n",
      "Epoch 67/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3473 - acc: 0.8546     \n",
      "Epoch 68/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3369 - acc: 0.8610     \n",
      "Epoch 69/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3274 - acc: 0.8646     \n",
      "Epoch 70/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3266 - acc: 0.8648     \n",
      "Epoch 71/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3422 - acc: 0.8547     - ETA: 0s - loss: 0.3371 - acc: \n",
      "Epoch 72/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3191 - acc: 0.8678     \n",
      "Epoch 73/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3146 - acc: 0.8695     \n",
      "Epoch 74/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3269 - acc: 0.8642     \n",
      "Epoch 75/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3117 - acc: 0.8712     \n",
      "Epoch 76/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3087 - acc: 0.8728     \n",
      "Epoch 77/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3173 - acc: 0.8700     \n",
      "Epoch 78/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3086 - acc: 0.8719     \n",
      "Epoch 79/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3104 - acc: 0.8718     \n",
      "Epoch 80/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3059 - acc: 0.8734     \n",
      "Epoch 81/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2993 - acc: 0.8777     \n",
      "Epoch 82/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3125 - acc: 0.8683     \n",
      "Epoch 83/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3019 - acc: 0.8748     \n",
      "Epoch 84/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2976 - acc: 0.8785     \n",
      "Epoch 85/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2966 - acc: 0.8786     \n",
      "Epoch 86/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2946 - acc: 0.8774     \n",
      "Epoch 87/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2930 - acc: 0.8777     \n",
      "Epoch 88/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2890 - acc: 0.8828     \n",
      "Epoch 89/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2911 - acc: 0.8795     \n",
      "Epoch 90/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2888 - acc: 0.8832     \n",
      "Epoch 91/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2897 - acc: 0.8808     \n",
      "Epoch 92/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2801 - acc: 0.8832     \n",
      "Epoch 93/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2833 - acc: 0.8852     \n",
      "Epoch 94/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2763 - acc: 0.8864     \n",
      "Epoch 95/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2799 - acc: 0.8843     \n",
      "Epoch 96/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2793 - acc: 0.8845     \n",
      "Epoch 97/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2762 - acc: 0.8875     \n",
      "Epoch 98/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2785 - acc: 0.8850     \n",
      "Epoch 99/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2735 - acc: 0.8888     \n",
      "Epoch 100/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2740 - acc: 0.8894     \n",
      "Epoch 101/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2592 - acc: 0.8937     \n",
      "Epoch 102/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2653 - acc: 0.8919     \n",
      "Epoch 103/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2756 - acc: 0.8865     \n",
      "Epoch 104/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2569 - acc: 0.8948     \n",
      "Epoch 105/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2714 - acc: 0.8879     \n",
      "Epoch 106/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2664 - acc: 0.8897     \n",
      "Epoch 107/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2672 - acc: 0.8883     \n",
      "Epoch 108/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2733 - acc: 0.8868     \n",
      "Epoch 109/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2541 - acc: 0.8963     \n",
      "Epoch 110/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2721 - acc: 0.8882     \n",
      "Epoch 111/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2505 - acc: 0.8972     \n",
      "Epoch 112/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2487 - acc: 0.8991     \n",
      "Epoch 113/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2452 - acc: 0.8989     \n",
      "Epoch 114/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2559 - acc: 0.8948     \n",
      "Epoch 115/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2401 - acc: 0.9034     \n",
      "Epoch 116/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2474 - acc: 0.8987     \n",
      "Epoch 117/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2462 - acc: 0.9003     \n",
      "Epoch 118/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2446 - acc: 0.8974     \n",
      "Epoch 119/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2687 - acc: 0.8888     \n",
      "Epoch 120/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2377 - acc: 0.9033     \n",
      "Epoch 121/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2461 - acc: 0.8991     \n",
      "Epoch 122/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2355 - acc: 0.9023     - ETA: 0s - loss: 0.2330 - acc: 0.9\n",
      "Epoch 123/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2524 - acc: 0.8978     \n",
      "Epoch 124/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2412 - acc: 0.8995     \n",
      "Epoch 125/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2385 - acc: 0.9020     \n",
      "Epoch 126/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2419 - acc: 0.9011     \n",
      "Epoch 127/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2346 - acc: 0.9036     \n",
      "Epoch 128/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2342 - acc: 0.9033     \n",
      "Epoch 129/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2380 - acc: 0.9018     \n",
      "Epoch 130/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2322 - acc: 0.9048     \n",
      "Epoch 131/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2304 - acc: 0.9062     \n",
      "Epoch 132/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2361 - acc: 0.9047     \n",
      "Epoch 133/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2278 - acc: 0.9067     \n",
      "Epoch 134/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2317 - acc: 0.9069     \n",
      "Epoch 135/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2372 - acc: 0.9034     \n",
      "Epoch 136/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2299 - acc: 0.9049     \n",
      "Epoch 137/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2363 - acc: 0.9035     \n",
      "Epoch 138/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2280 - acc: 0.9074     \n",
      "Epoch 139/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2140 - acc: 0.9110     \n",
      "Epoch 140/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2221 - acc: 0.9092     \n",
      "Epoch 141/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2251 - acc: 0.9074     \n",
      "Epoch 142/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2194 - acc: 0.9093     \n",
      "Epoch 143/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2229 - acc: 0.9068     \n",
      "Epoch 144/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2120 - acc: 0.9125     \n",
      "Epoch 145/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2152 - acc: 0.9090     \n",
      "Epoch 146/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2249 - acc: 0.9090     \n",
      "Epoch 147/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2091 - acc: 0.9138     \n",
      "Epoch 148/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2109 - acc: 0.9121     \n",
      "Epoch 149/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2182 - acc: 0.9106     \n",
      "Epoch 150/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2043 - acc: 0.9158     \n",
      "Epoch 151/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2067 - acc: 0.9146     \n",
      "Epoch 152/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2091 - acc: 0.9142     \n",
      "Epoch 153/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2062 - acc: 0.9149     \n",
      "Epoch 154/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2012 - acc: 0.9175     \n",
      "Epoch 155/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2062 - acc: 0.9121     \n",
      "Epoch 156/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2066 - acc: 0.9130     \n",
      "Epoch 157/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2104 - acc: 0.9128     \n",
      "Epoch 158/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2011 - acc: 0.9146     \n",
      "Epoch 159/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2107 - acc: 0.9148     \n",
      "Epoch 160/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1961 - acc: 0.9191     \n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s - loss: 0.1941 - acc: 0.9186     \n",
      "Epoch 162/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1927 - acc: 0.9186     \n",
      "Epoch 163/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1884 - acc: 0.9209     \n",
      "Epoch 164/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1905 - acc: 0.9233     \n",
      "Epoch 165/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1885 - acc: 0.9219     \n",
      "Epoch 166/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1883 - acc: 0.9209     \n",
      "Epoch 167/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1866 - acc: 0.9218     \n",
      "Epoch 168/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1941 - acc: 0.9206     \n",
      "Epoch 169/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1946 - acc: 0.9193     \n",
      "Epoch 170/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1877 - acc: 0.9230     \n",
      "Epoch 171/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1841 - acc: 0.9250     \n",
      "Epoch 172/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1826 - acc: 0.9241     \n",
      "Epoch 173/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1935 - acc: 0.9208     \n",
      "Epoch 174/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1931 - acc: 0.9190     \n",
      "Epoch 175/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1706 - acc: 0.9316     \n",
      "Epoch 176/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1676 - acc: 0.9300     \n",
      "Epoch 177/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1796 - acc: 0.9249     \n",
      "Epoch 178/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1840 - acc: 0.9218     \n",
      "Epoch 179/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1787 - acc: 0.9247     \n",
      "Epoch 180/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1823 - acc: 0.9244     \n",
      "Epoch 181/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1755 - acc: 0.9270     \n",
      "Epoch 182/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1677 - acc: 0.9310     \n",
      "Epoch 183/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1639 - acc: 0.9326     \n",
      "Epoch 184/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1627 - acc: 0.9325     \n",
      "Epoch 185/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1712 - acc: 0.9293     \n",
      "Epoch 186/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1578 - acc: 0.9321     \n",
      "Epoch 187/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1608 - acc: 0.9324     \n",
      "Epoch 188/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1687 - acc: 0.9294     \n",
      "Epoch 189/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1543 - acc: 0.9341     \n",
      "Epoch 190/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1656 - acc: 0.9306     \n",
      "Epoch 191/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1590 - acc: 0.9332     \n",
      "Epoch 192/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1575 - acc: 0.9347     \n",
      "Epoch 193/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1449 - acc: 0.9380     \n",
      "Epoch 194/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1571 - acc: 0.9354     \n",
      "Epoch 195/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1490 - acc: 0.9375     \n",
      "Epoch 196/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1642 - acc: 0.9327     \n",
      "Epoch 197/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1440 - acc: 0.9400     \n",
      "Epoch 198/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1529 - acc: 0.9378     \n",
      "Epoch 199/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1527 - acc: 0.9363     \n",
      "Epoch 200/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1464 - acc: 0.9402     \n",
      "Epoch 201/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1747 - acc: 0.9281     \n",
      "Epoch 202/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1382 - acc: 0.9420     \n",
      "Epoch 203/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1409 - acc: 0.9409     \n",
      "Epoch 204/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1450 - acc: 0.9396     \n",
      "Epoch 205/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1326 - acc: 0.9432     \n",
      "Epoch 206/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1342 - acc: 0.9449     \n",
      "Epoch 207/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1449 - acc: 0.9395     \n",
      "Epoch 208/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1347 - acc: 0.9435     \n",
      "Epoch 209/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1305 - acc: 0.9466     \n",
      "Epoch 210/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1227 - acc: 0.9493     \n",
      "Epoch 211/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1436 - acc: 0.9408     \n",
      "Epoch 212/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1253 - acc: 0.9490     \n",
      "Epoch 213/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1379 - acc: 0.9429     \n",
      "Epoch 214/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1285 - acc: 0.9472     \n",
      "Epoch 215/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1201 - acc: 0.9481     \n",
      "Epoch 216/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1258 - acc: 0.9482     \n",
      "Epoch 217/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1281 - acc: 0.9468     \n",
      "Epoch 218/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1205 - acc: 0.9475     \n",
      "Epoch 219/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1180 - acc: 0.9513     \n",
      "Epoch 220/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1190 - acc: 0.9502     \n",
      "Epoch 221/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1181 - acc: 0.9504     \n",
      "Epoch 222/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1247 - acc: 0.9484     \n",
      "Epoch 223/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1252 - acc: 0.9494     \n",
      "Epoch 224/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1346 - acc: 0.9460     \n",
      "Epoch 225/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1217 - acc: 0.9493     \n",
      "Epoch 226/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1156 - acc: 0.9526     \n",
      "Epoch 227/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1113 - acc: 0.9549     \n",
      "Epoch 228/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1062 - acc: 0.9572     \n",
      "Epoch 229/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1062 - acc: 0.9558     \n",
      "Epoch 230/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1024 - acc: 0.9596     \n",
      "Epoch 231/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1209 - acc: 0.9514     \n",
      "Epoch 232/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1124 - acc: 0.9536     \n",
      "Epoch 233/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1029 - acc: 0.9590     \n",
      "Epoch 234/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0985 - acc: 0.9606     \n",
      "Epoch 235/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1018 - acc: 0.9589     \n",
      "Epoch 236/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0974 - acc: 0.9599     \n",
      "Epoch 237/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0993 - acc: 0.9590     \n",
      "Epoch 238/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1021 - acc: 0.9583     \n",
      "Epoch 239/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0983 - acc: 0.9598     \n",
      "Epoch 240/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0880 - acc: 0.9646     \n",
      "Epoch 241/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1044 - acc: 0.9596     \n",
      "Epoch 242/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0913 - acc: 0.9645     \n",
      "Epoch 243/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0930 - acc: 0.9622     \n",
      "Epoch 244/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0906 - acc: 0.9636     \n",
      "Epoch 245/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0879 - acc: 0.9639     \n",
      "Epoch 246/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0893 - acc: 0.9618     \n",
      "Epoch 247/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0992 - acc: 0.9605     \n",
      "Epoch 248/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0959 - acc: 0.9614     \n",
      "Epoch 249/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0939 - acc: 0.9632     \n",
      "Epoch 250/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1029 - acc: 0.9590     \n",
      "Epoch 251/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0772 - acc: 0.9702     \n",
      "Epoch 252/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0844 - acc: 0.9653     \n",
      "Epoch 253/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0780 - acc: 0.9695     \n",
      "Epoch 254/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0799 - acc: 0.9668     \n",
      "Epoch 255/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0770 - acc: 0.9696     \n",
      "Epoch 256/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0788 - acc: 0.9691     \n",
      "Epoch 257/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0826 - acc: 0.9664     \n",
      "Epoch 258/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0880 - acc: 0.9670     \n",
      "Epoch 259/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0766 - acc: 0.9697     \n",
      "Epoch 260/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0771 - acc: 0.9694     \n",
      "Epoch 261/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0833 - acc: 0.9673     \n",
      "Epoch 262/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0689 - acc: 0.9706     \n",
      "Epoch 263/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0789 - acc: 0.9671     \n",
      "Epoch 264/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0742 - acc: 0.9727     \n",
      "Epoch 265/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0788 - acc: 0.9698     \n",
      "Epoch 266/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0770 - acc: 0.9699     \n",
      "Epoch 267/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0724 - acc: 0.9708     \n",
      "Epoch 268/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0790 - acc: 0.9691     \n",
      "Epoch 269/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0698 - acc: 0.9724     \n",
      "Epoch 270/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0711 - acc: 0.9719     \n",
      "Epoch 271/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0730 - acc: 0.9720     \n",
      "Epoch 272/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0756 - acc: 0.9712     - ETA: 0s - loss: 0.0759 - acc: 0.971\n",
      "Epoch 273/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0695 - acc: 0.9719     \n",
      "Epoch 274/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0646 - acc: 0.9748     \n",
      "Epoch 275/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0926 - acc: 0.9645     \n",
      "Epoch 276/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0618 - acc: 0.9763     \n",
      "Epoch 277/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0782 - acc: 0.9695     \n",
      "Epoch 278/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0585 - acc: 0.9785     \n",
      "Epoch 279/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0772 - acc: 0.9712     \n",
      "Epoch 280/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0740 - acc: 0.9698     \n",
      "Epoch 281/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0583 - acc: 0.9788     \n",
      "Epoch 282/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0883 - acc: 0.9674     \n",
      "Epoch 283/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0613 - acc: 0.9760     \n",
      "Epoch 284/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0649 - acc: 0.9748     \n",
      "Epoch 285/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0625 - acc: 0.9758     \n",
      "Epoch 286/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0774 - acc: 0.9713     \n",
      "Epoch 287/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0575 - acc: 0.9779     \n",
      "Epoch 288/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0639 - acc: 0.9761     \n",
      "Epoch 289/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0808 - acc: 0.9690     \n",
      "Epoch 290/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0503 - acc: 0.9815     \n",
      "Epoch 291/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0614 - acc: 0.9772     \n",
      "Epoch 292/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0563 - acc: 0.9796     \n",
      "Epoch 293/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0539 - acc: 0.9798     \n",
      "Epoch 294/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0611 - acc: 0.9762     \n",
      "Epoch 295/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0555 - acc: 0.9795     \n",
      "Epoch 296/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0540 - acc: 0.9787     \n",
      "Epoch 297/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0608 - acc: 0.9773     \n",
      "Epoch 298/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0581 - acc: 0.9791     \n",
      "Epoch 299/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0449 - acc: 0.9832     \n",
      "Epoch 300/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0600 - acc: 0.9758     \n",
      "3744/3760 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8797060686857142, 0.8521276595744681]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set epochs and everything\n",
    "learningRate=0.0008\n",
    "epochNum=300\n",
    "#epochNum=100   # using shorter for faster training\n",
    "batchSize=480  # best was 475 but 480 is a multiple of 32, fits in memory better?\n",
    "\n",
    "adam=keras.optimizers.Adam(beta_1=0.9, beta_2=0.999,lr=learningRate)\n",
    "Model1 = modelv2(X_train.shape[1:] )\n",
    "Model1.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "Model1.fit(x = X_train, y = Y_train, epochs = epochNum, batch_size = batchSize,  verbose=1)\n",
    "Model1.evaluate(x = X_test, y = Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3760/3760 [==============================] - 0s     \n",
      "3552/3760 [===========================>..] - ETA: 0s\n",
      " Final ensemble test accuracy of:  87.39 , mean test accuracy of: 84.41 with a std of:  0.61\n"
     ]
    }
   ],
   "source": [
    "# set epochs and everything\n",
    "learningRate=0.0008\n",
    "epochNum=250\n",
    "#epochNum=100   # using shorter for faster training\n",
    "batchSize=480  # best was 475 but 480 is a multiple of 32, fits in memory better?\n",
    "adam=keras.optimizers.Adam(beta_1=0.9, beta_2=0.999,lr=learningRate)\n",
    "test_acc=np.zeros((5,1))\n",
    "\n",
    "# make a list of models\n",
    "ensembleSize=5\n",
    "Models=[]\n",
    "\n",
    "# train the ensemble of models\n",
    "for i in range(ensembleSize):\n",
    "    \n",
    "    # make the model\n",
    "    Models.append(modelv2(X_train.shape[1:] ))\n",
    "    \n",
    "    # compile the model\n",
    "    Models[i].compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    " \n",
    "    # fit the model\n",
    "    Models[i].fit(x = X_train, y = Y_train, epochs = epochNum, batch_size = batchSize,  verbose=0)\n",
    "\n",
    "    # predict\n",
    "    temp=Models[i].evaluate(x = X_test, y = Y_test)\n",
    "    test_acc[i]=temp[1]\n",
    "\n",
    "test_acc_var=np.var(100*test_acc)\n",
    "\n",
    "# get the mean label, round it and find the emsemble test acc\n",
    "meanLabels=(Models[0].predict(X_test)+Models[1].predict(X_test)+Models[2].predict(X_test)+Models[3].predict(X_test)+Models[4].predict(X_test))/5\n",
    "meanLabels=np.round(meanLabels)\n",
    "wrongLabels=Y_test-meanLabels\n",
    "test_acc_mean=1-np.sum(np.abs(wrongLabels))/len(Y_test)\n",
    "\n",
    "print(\"\\n Final ensemble test accuracy of: \",str(np.round(test_acc_mean*100,2)),\", mean test accuracy of:\",np.str(np.round(np.mean(test_acc)*100,2)),\"with a std of: \",str(np.round(test_acc_var,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier init for the convs got 87%, trying he instead, got 85, do xavier I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataClasses = [\"Bad Data\",\"Good Data\"]\n",
    "\n",
    "\n",
    "confusionMat = sklearn.metrics.confusion_matrix(Y_test,meanLabels)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(confusionMat)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(dataClasses)))\n",
    "ax.set_yticks(np.arange(len(dataClasses)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(dataClasses)\n",
    "ax.set_yticklabels(dataClasses)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(dataClasses)):\n",
    "    for j in range(len(dataClasses)):\n",
    "        text = ax.text(j, i, confusionMat[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "#x.set_title(\"Harvest of local farmers (in tons/year)\")\n",
    "fig.tight_layout()\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper paramater tuning, currently just set up for learning rate, batch size and # of epochs\n",
    "\n",
    "# values already tried, Feb 19th\n",
    "#lr_array=np.array([0.1,0.05,0.01,0.005,0.001])\n",
    "#batch_size_array=np.array([16,32,64,128,256,512,1024])\n",
    "#epoch_array=np.array([10,20,40,80,160])\n",
    "\n",
    "# set the arrays Feb 19th, best train acc was at 0.0008 200 450, best test was at 0.0008 160 450\n",
    "#lr_array=np.array([0.005,0.003,0.001,0.0008,0.0005,0.0001])\n",
    "#batch_size_array=np.array([400,450,500,512,550])\n",
    "#epoch_array=np.array([100,130,160,180,200])\n",
    "\n",
    "# set the arrays, best test acc was 85%, train acc of 97% at 0.0008 250 475 (got to 100% train acc with longer train time, 500 epochs)\n",
    "lr_array=np.array([0.0009,0.0008,0.0007,0.0006])\n",
    "batch_size_array=np.array([425,450,475])\n",
    "epoch_array=np.array([160,170,250,500])\n",
    "\n",
    "\n",
    "# get their sizes and their final size\n",
    "lr_array_size=np.shape(lr_array)[0]\n",
    "batch_size_array_size=np.shape(batch_size_array)[0]\n",
    "epoch_array_size=np.shape(epoch_array)[0]\n",
    "final_size=lr_array_size*batch_size_array_size*epoch_array_size\n",
    "\n",
    "# tile and reshape them\n",
    "lr_array=np.tile(lr_array,final_size).squeeze()\n",
    "batch_size_array=np.reshape(np.tile(np.tile(batch_size_array,(lr_array_size,1)).T,(epoch_array_size,1)),[final_size,1]).squeeze()\n",
    "epoch_array=np.reshape(np.tile(epoch_array,(lr_array_size*batch_size_array_size,1)).T,[1,final_size]).squeeze()\n",
    "\n",
    "# make lists and arrays for the results\n",
    "training_history=[]\n",
    "training_history_acc=np.zeros((final_size,1))\n",
    "preds_history=[]\n",
    "preds_loss=np.zeros((final_size,1))\n",
    "preds_acc=np.zeros((final_size,1))\n",
    "\n",
    "# loop de loop\n",
    "for i in range(final_size):\n",
    "    \n",
    "    # make the model and its opimizer\n",
    "    happyModel = model(X_train.shape[1:] )\n",
    "    adam=keras.optimizers.Adam(beta_1=0.9, beta_2=0.999,lr=lr_array[i])\n",
    "    \n",
    "    # compile the model\n",
    "    happyModel.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    # fit the model\n",
    "    temp=happyModel.fit(x = X_train, y = Y_train, epochs = np.int(epoch_array[i]), batch_size = np.int(batch_size_array[i]),  verbose=0)\n",
    "    \n",
    "    # store how it did\n",
    "    training_history_acc[i]=temp.history['acc'][-1]\n",
    "    training_history.append(temp)\n",
    "    \n",
    "    # predict and store on x_test\n",
    "    temp=happyModel.evaluate(x = X_test, y = Y_test)\n",
    "    preds_loss[i]=temp[0]\n",
    "    preds_acc[i]=temp[1]\n",
    "    preds_history.append(temp)\n",
    "    \n",
    "    # print a msg\n",
    "    print(\" Training accuracy = \"+str(np.round(training_history_acc[i]*100,2))+\"%, learning rate, epoch size, and mini-batch size: \"+str(lr_array[i])+\",\"+str(epoch_array[i])+\",\"+str(batch_size_array[i])+\", test loss and accuracy: \"+str(preds_loss[i])+str(np.round(preds_acc[i]*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.456235956638418, 0.7204787234042553]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best is lr=0.0008, epochs=250, batch size=475, gives 97.4% train and 85.5% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8550531914893617 0.0008 250 475\n"
     ]
    }
   ],
   "source": [
    "best=np.argmax(preds_acc)\n",
    "print(np.max(preds_acc),lr_array[best],epoch_array[best],batch_size_array[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0009 500 425\n"
     ]
    }
   ],
   "source": [
    "best=np.argmax(training_history_acc)\n",
    "print(np.max(training_history_acc),lr_array[best],epoch_array[best],batch_size_array[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680/3760 [============================>.] - ETA: 0s\n",
      "Loss = 0.4233942541670292\n",
      "Test Accuracy = 0.8236702127659574\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds = happyModel.evaluate(x = X_test, y = Y_test)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_56 (Conv2D)           (None, 3, 37, 16)         304       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_56 (MaxPooling (None, 3, 18, 16)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)   (None, 3, 18, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 3, 18, 32)         7712      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling (None, 3, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)   (None, 3, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 3, 9, 64)          43072     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)   (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 3, 4, 128)         221312    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 3, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 3, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 3, 2, 256)         1081600   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 3, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 3, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 256)               196864    \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 160)               41120     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 96)                15456     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 32)                3104      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,610,577\n",
      "Trainable params: 1,610,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1724pt\" viewBox=\"0.00 0.00 223.00 1724.00\" width=\"223pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1720)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1720 219,-1720 219,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 1961664168344 -->\n",
       "<g class=\"node\" id=\"node1\"><title>1961664168344</title>\n",
       "<polygon fill=\"none\" points=\"16.5,-1679.5 16.5,-1715.5 198.5,-1715.5 198.5,-1679.5 16.5,-1679.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1693.8\">conv2d_56_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 1961664168064 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1961664168064</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-1606.5 40.5,-1642.5 174.5,-1642.5 174.5,-1606.5 40.5,-1606.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1620.8\">conv2d_56: Conv2D</text>\n",
       "</g>\n",
       "<!-- 1961664168344&#45;&gt;1961664168064 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>1961664168344-&gt;1961664168064</title>\n",
       "<path d=\"M107.5,-1679.31C107.5,-1671.29 107.5,-1661.55 107.5,-1652.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1652.53 107.5,-1642.53 104,-1652.53 111,-1652.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664185960 -->\n",
       "<g class=\"node\" id=\"node3\"><title>1961664185960</title>\n",
       "<polygon fill=\"none\" points=\"0,-1533.5 0,-1569.5 215,-1569.5 215,-1533.5 0,-1533.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1547.8\">max_pooling2d_56: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 1961664168064&#45;&gt;1961664185960 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1961664168064-&gt;1961664185960</title>\n",
       "<path d=\"M107.5,-1606.31C107.5,-1598.29 107.5,-1588.55 107.5,-1579.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1579.53 107.5,-1569.53 104,-1579.53 111,-1579.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664188032 -->\n",
       "<g class=\"node\" id=\"node4\"><title>1961664188032</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-1460.5 19.5,-1496.5 195.5,-1496.5 195.5,-1460.5 19.5,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1474.8\">leaky_re_lu_56: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 1961664185960&#45;&gt;1961664188032 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>1961664185960-&gt;1961664188032</title>\n",
       "<path d=\"M107.5,-1533.31C107.5,-1525.29 107.5,-1515.55 107.5,-1506.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1506.53 107.5,-1496.53 104,-1506.53 111,-1506.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664188144 -->\n",
       "<g class=\"node\" id=\"node5\"><title>1961664188144</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-1387.5 40.5,-1423.5 174.5,-1423.5 174.5,-1387.5 40.5,-1387.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1401.8\">conv2d_57: Conv2D</text>\n",
       "</g>\n",
       "<!-- 1961664188032&#45;&gt;1961664188144 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>1961664188032-&gt;1961664188144</title>\n",
       "<path d=\"M107.5,-1460.31C107.5,-1452.29 107.5,-1442.55 107.5,-1433.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1433.53 107.5,-1423.53 104,-1433.53 111,-1433.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664118400 -->\n",
       "<g class=\"node\" id=\"node6\"><title>1961664118400</title>\n",
       "<polygon fill=\"none\" points=\"0,-1314.5 0,-1350.5 215,-1350.5 215,-1314.5 0,-1314.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1328.8\">max_pooling2d_57: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 1961664188144&#45;&gt;1961664118400 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1961664188144-&gt;1961664118400</title>\n",
       "<path d=\"M107.5,-1387.31C107.5,-1379.29 107.5,-1369.55 107.5,-1360.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1360.53 107.5,-1350.53 104,-1360.53 111,-1360.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664370448 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1961664370448</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-1241.5 19.5,-1277.5 195.5,-1277.5 195.5,-1241.5 19.5,-1241.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1255.8\">leaky_re_lu_57: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 1961664118400&#45;&gt;1961664370448 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>1961664118400-&gt;1961664370448</title>\n",
       "<path d=\"M107.5,-1314.31C107.5,-1306.29 107.5,-1296.55 107.5,-1287.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1287.53 107.5,-1277.53 104,-1287.53 111,-1287.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664470600 -->\n",
       "<g class=\"node\" id=\"node8\"><title>1961664470600</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-1168.5 40.5,-1204.5 174.5,-1204.5 174.5,-1168.5 40.5,-1168.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1182.8\">conv2d_58: Conv2D</text>\n",
       "</g>\n",
       "<!-- 1961664370448&#45;&gt;1961664470600 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>1961664370448-&gt;1961664470600</title>\n",
       "<path d=\"M107.5,-1241.31C107.5,-1233.29 107.5,-1223.55 107.5,-1214.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1214.53 107.5,-1204.53 104,-1214.53 111,-1214.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664470544 -->\n",
       "<g class=\"node\" id=\"node9\"><title>1961664470544</title>\n",
       "<polygon fill=\"none\" points=\"0,-1095.5 0,-1131.5 215,-1131.5 215,-1095.5 0,-1095.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1109.8\">max_pooling2d_58: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 1961664470600&#45;&gt;1961664470544 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1961664470600-&gt;1961664470544</title>\n",
       "<path d=\"M107.5,-1168.31C107.5,-1160.29 107.5,-1150.55 107.5,-1141.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1141.53 107.5,-1131.53 104,-1141.53 111,-1141.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664652344 -->\n",
       "<g class=\"node\" id=\"node10\"><title>1961664652344</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-1022.5 19.5,-1058.5 195.5,-1058.5 195.5,-1022.5 19.5,-1022.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1036.8\">leaky_re_lu_58: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 1961664470544&#45;&gt;1961664652344 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>1961664470544-&gt;1961664652344</title>\n",
       "<path d=\"M107.5,-1095.31C107.5,-1087.29 107.5,-1077.55 107.5,-1068.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1068.53 107.5,-1058.53 104,-1068.53 111,-1068.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664738696 -->\n",
       "<g class=\"node\" id=\"node11\"><title>1961664738696</title>\n",
       "<polygon fill=\"none\" points=\"39,-949.5 39,-985.5 176,-985.5 176,-949.5 39,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-963.8\">dropout_23: Dropout</text>\n",
       "</g>\n",
       "<!-- 1961664652344&#45;&gt;1961664738696 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>1961664652344-&gt;1961664738696</title>\n",
       "<path d=\"M107.5,-1022.31C107.5,-1014.29 107.5,-1004.55 107.5,-995.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-995.529 107.5,-985.529 104,-995.529 111,-995.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664655088 -->\n",
       "<g class=\"node\" id=\"node12\"><title>1961664655088</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-876.5 40.5,-912.5 174.5,-912.5 174.5,-876.5 40.5,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-890.8\">conv2d_59: Conv2D</text>\n",
       "</g>\n",
       "<!-- 1961664738696&#45;&gt;1961664655088 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>1961664738696-&gt;1961664655088</title>\n",
       "<path d=\"M107.5,-949.313C107.5,-941.289 107.5,-931.547 107.5,-922.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-922.529 107.5,-912.529 104,-922.529 111,-922.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664654640 -->\n",
       "<g class=\"node\" id=\"node13\"><title>1961664654640</title>\n",
       "<polygon fill=\"none\" points=\"0,-803.5 0,-839.5 215,-839.5 215,-803.5 0,-803.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-817.8\">max_pooling2d_59: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 1961664655088&#45;&gt;1961664654640 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>1961664655088-&gt;1961664654640</title>\n",
       "<path d=\"M107.5,-876.313C107.5,-868.289 107.5,-858.547 107.5,-849.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-849.529 107.5,-839.529 104,-849.529 111,-849.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961664832008 -->\n",
       "<g class=\"node\" id=\"node14\"><title>1961664832008</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-730.5 19.5,-766.5 195.5,-766.5 195.5,-730.5 19.5,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-744.8\">leaky_re_lu_59: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 1961664654640&#45;&gt;1961664832008 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>1961664654640-&gt;1961664832008</title>\n",
       "<path d=\"M107.5,-803.313C107.5,-795.289 107.5,-785.547 107.5,-776.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-776.529 107.5,-766.529 104,-776.529 111,-776.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665129944 -->\n",
       "<g class=\"node\" id=\"node15\"><title>1961665129944</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-657.5 40.5,-693.5 174.5,-693.5 174.5,-657.5 40.5,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-671.8\">conv2d_60: Conv2D</text>\n",
       "</g>\n",
       "<!-- 1961664832008&#45;&gt;1961665129944 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>1961664832008-&gt;1961665129944</title>\n",
       "<path d=\"M107.5,-730.313C107.5,-722.289 107.5,-712.547 107.5,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-703.529 107.5,-693.529 104,-703.529 111,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665129888 -->\n",
       "<g class=\"node\" id=\"node16\"><title>1961665129888</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 215,-620.5 215,-584.5 0,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-598.8\">max_pooling2d_60: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 1961665129944&#45;&gt;1961665129888 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>1961665129944-&gt;1961665129888</title>\n",
       "<path d=\"M107.5,-657.313C107.5,-649.289 107.5,-639.547 107.5,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-630.529 107.5,-620.529 104,-630.529 111,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665299400 -->\n",
       "<g class=\"node\" id=\"node17\"><title>1961665299400</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-511.5 19.5,-547.5 195.5,-547.5 195.5,-511.5 19.5,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-525.8\">leaky_re_lu_60: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 1961665129888&#45;&gt;1961665299400 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>1961665129888-&gt;1961665299400</title>\n",
       "<path d=\"M107.5,-584.313C107.5,-576.289 107.5,-566.547 107.5,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-557.529 107.5,-547.529 104,-557.529 111,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665393944 -->\n",
       "<g class=\"node\" id=\"node18\"><title>1961665393944</title>\n",
       "<polygon fill=\"none\" points=\"49.5,-438.5 49.5,-474.5 165.5,-474.5 165.5,-438.5 49.5,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-452.8\">flatten_12: Flatten</text>\n",
       "</g>\n",
       "<!-- 1961665299400&#45;&gt;1961665393944 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>1961665299400-&gt;1961665393944</title>\n",
       "<path d=\"M107.5,-511.313C107.5,-503.289 107.5,-493.547 107.5,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-484.529 107.5,-474.529 104,-484.529 111,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665301696 -->\n",
       "<g class=\"node\" id=\"node19\"><title>1961665301696</title>\n",
       "<polygon fill=\"none\" points=\"52,-365.5 52,-401.5 163,-401.5 163,-365.5 52,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-379.8\">dense_56: Dense</text>\n",
       "</g>\n",
       "<!-- 1961665393944&#45;&gt;1961665301696 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>1961665393944-&gt;1961665301696</title>\n",
       "<path d=\"M107.5,-438.313C107.5,-430.289 107.5,-420.547 107.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-411.529 107.5,-401.529 104,-411.529 111,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665571192 -->\n",
       "<g class=\"node\" id=\"node20\"><title>1961665571192</title>\n",
       "<polygon fill=\"none\" points=\"52,-292.5 52,-328.5 163,-328.5 163,-292.5 52,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-306.8\">dense_57: Dense</text>\n",
       "</g>\n",
       "<!-- 1961665301696&#45;&gt;1961665571192 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>1961665301696-&gt;1961665571192</title>\n",
       "<path d=\"M107.5,-365.313C107.5,-357.289 107.5,-347.547 107.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-338.529 107.5,-328.529 104,-338.529 111,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665689864 -->\n",
       "<g class=\"node\" id=\"node21\"><title>1961665689864</title>\n",
       "<polygon fill=\"none\" points=\"52,-219.5 52,-255.5 163,-255.5 163,-219.5 52,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-233.8\">dense_58: Dense</text>\n",
       "</g>\n",
       "<!-- 1961665571192&#45;&gt;1961665689864 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>1961665571192-&gt;1961665689864</title>\n",
       "<path d=\"M107.5,-292.313C107.5,-284.289 107.5,-274.547 107.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-265.529 107.5,-255.529 104,-265.529 111,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961665779920 -->\n",
       "<g class=\"node\" id=\"node22\"><title>1961665779920</title>\n",
       "<polygon fill=\"none\" points=\"52,-146.5 52,-182.5 163,-182.5 163,-146.5 52,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-160.8\">dense_59: Dense</text>\n",
       "</g>\n",
       "<!-- 1961665689864&#45;&gt;1961665779920 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>1961665689864-&gt;1961665779920</title>\n",
       "<path d=\"M107.5,-219.313C107.5,-211.289 107.5,-201.547 107.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-192.529 107.5,-182.529 104,-192.529 111,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961666008512 -->\n",
       "<g class=\"node\" id=\"node23\"><title>1961666008512</title>\n",
       "<polygon fill=\"none\" points=\"39,-73.5 39,-109.5 176,-109.5 176,-73.5 39,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-87.8\">dropout_24: Dropout</text>\n",
       "</g>\n",
       "<!-- 1961665779920&#45;&gt;1961666008512 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>1961665779920-&gt;1961666008512</title>\n",
       "<path d=\"M107.5,-146.313C107.5,-138.289 107.5,-128.547 107.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-119.529 107.5,-109.529 104,-119.529 111,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1961666007112 -->\n",
       "<g class=\"node\" id=\"node24\"><title>1961666007112</title>\n",
       "<polygon fill=\"none\" points=\"52,-0.5 52,-36.5 163,-36.5 163,-0.5 52,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-14.8\">dense_60: Dense</text>\n",
       "</g>\n",
       "<!-- 1961666008512&#45;&gt;1961666007112 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>1961666008512-&gt;1961666007112</title>\n",
       "<path d=\"M107.5,-73.3129C107.5,-65.2895 107.5,-55.5475 107.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-46.5288 107.5,-36.5288 104,-46.5289 111,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(Models[0], to_file='Model_graph.png')\n",
    "SVG(model_to_dot(Models[0]).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
