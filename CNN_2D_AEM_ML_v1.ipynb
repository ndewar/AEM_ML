{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "import pickle\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "data = pandas.read_pickle('raw_processed.pkl')\n",
    "\n",
    "# make a short version for better viewing\n",
    "dataShort = data.iloc[0:10,19:46].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x221b2fcb9e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.closest_pos.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\pandas\\core\\frame.py:3930: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235.0\n"
     ]
    }
   ],
   "source": [
    "# set the number of adjacent soundings to use\n",
    "adSoundings=1;\n",
    "\n",
    "# get the line numbers, find the time difference between adjacent rows, and set a threshold for two points to be considered \n",
    "# part of the same sounding\n",
    "lineNumbers=np.unique(data[\"LINE_NO\"])\n",
    "timeDiff=np.diff(data[\"TIMESTAMP\"])\n",
    "timeDiffMask=timeDiff<5e-6\n",
    "\n",
    "# iterate through the line numbers\n",
    "for line in lineNumbers:\n",
    "    \n",
    "# this is for debugging\n",
    "#if 1==1:\n",
    "#    line=lineNumbers[0]\n",
    "\n",
    "    # make a mask for the rows in the big DF that are for this line number\n",
    "    rowIndex=data[\"LINE_NO\"]==line\n",
    "    \n",
    "    # get just the current rows\n",
    "    currData=data.loc[rowIndex,:]\n",
    "    currData.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # make an array of the time differences\n",
    "    timeDiff=np.diff(currData[\"TIMESTAMP\"])\n",
    "\n",
    "    # if the first time difference is large, drop the first row, this throws a warning\n",
    "    if timeDiff[0]>1e-5:\n",
    "        currData.drop([0], inplace=True)\n",
    "        currData.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # set the number of pairs in this line and make some arrays with large numbers for the HM and LM data\n",
    "    numberPairs=np.int(len(currData.index)/2)\n",
    "    currDataLM=np.ones((numberPairs,1+adSoundings*2,37))*9000\n",
    "    currDataHM=np.ones((numberPairs,1+adSoundings*2,37))*9000\n",
    "    currLabelsLM=np.zeros((numberPairs,1))\n",
    "    currLabelsHM=np.zeros((numberPairs,1))\n",
    "    \n",
    "    # check to see if the first row is LM or HM, set the indexing\n",
    "    if np.mean(currData.loc[0,'DBDT_Ch1GT1':'DBDT_Ch1GT28'])>9990:\n",
    "        hmFirst=1\n",
    "    else:\n",
    "        hmFirst=0\n",
    "               \n",
    "    # iterate through the number of pairs\n",
    "    for i in range(numberPairs):\n",
    "        if hmFirst==1:\n",
    "            if adSoundings==1:\n",
    "                hmIndex=[(i-1)*2,i*2,(i+1)*2]\n",
    "                lmIndex=[(i-1)*2+1,i*2+1,(i+1)*2+1]\n",
    "            elif adSoundings==2:\n",
    "                hmIndex=[(i-2)*2,(i-1)*2,i*2,(i+1)*2,(i+2)*2]\n",
    "                lmIndex=[(i-2)*2+1,(i-1)*2+1,i*2+1,(i+1)*2+1,(i+2)*2+1]\n",
    "        else:\n",
    "            if adSoundings==1:\n",
    "                lmIndex=[(i-1)*2,i*2,(i+1)*2]\n",
    "                hmIndex=[(i-1)*2+1,i*2+1,(i+1)*2+1]\n",
    "            elif adSoundings==2:\n",
    "                hmIndex=[(i-2)*2,(i-1)*2,i*2,(i+1)*2,(i+2)*2]\n",
    "                lmIndex=[(i-2)*2+1,(i-1)*2+1,i*2+1,(i+1)*2+1,(i+2)*2+1]\n",
    "        \n",
    "        # for the first pair, leave the sounding in position 0 alone as it doesnt exist, do the same for the last pair but \n",
    "        # for the sounding in position 2\n",
    "        # TODO: modify this to work for 2 or 3 or 4 or 5 adjacent soundings\n",
    "        if i==0:\n",
    "            currDataLM[i,2,0:28]=currData.loc[lmIndex[2],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataHM[i,2,:]=currData.loc[hmIndex[2],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "        elif i==numberPairs-1:\n",
    "            currDataLM[i,0,0:28]=currData.loc[lmIndex[0],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataHM[i,0,:]=currData.loc[hmIndex[0],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "            \n",
    "        # for every other pair fill in both adjacent soundings\n",
    "        # TODO: modify this to work for 2 or 3 or 4 or 5 adjacent soundings\n",
    "        else:\n",
    "            currDataLM[i,0,0:28]=currData.loc[lmIndex[0],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataLM[i,2,0:28]=currData.loc[lmIndex[2],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "            currDataHM[i,0,:]=currData.loc[hmIndex[0],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "            currDataHM[i,2,:]=currData.loc[hmIndex[2],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "            \n",
    "        # middle sounding and labels always get set for every pair\n",
    "        currDataLM[i,1,0:28]=currData.loc[lmIndex[1],'DBDT_Ch1GT1':'DBDT_Ch1GT28']\n",
    "        currDataHM[i,1,:]=currData.loc[hmIndex[1],'DBDT_Ch2GT1':'DBDT_Ch2GT37']\n",
    "        currLabelsLM[i,0]=currData.loc[i*2+1,'VALID']\n",
    "        currLabelsHM[i,0]=currData.loc[i*2,'VALID']\n",
    "    \n",
    "    # set large values to mean of other values\n",
    "    for k in range(0,37):\n",
    "        \n",
    "        # get the current timegate\n",
    "        tempDataHM=currDataHM[:,:,k]\n",
    "        \n",
    "        # set values over 5000 to the mean of the timegate\n",
    "        tempDataHM[np.abs(tempDataHM)>5000]=np.mean(tempDataHM[np.abs(tempDataHM)<5000])\n",
    "        \n",
    "        # do the same to nans\n",
    "        tempDataHM=np.nan_to_num(tempDataHM, nan=np.mean(tempDataHM[np.abs(tempDataHM)<5000]))\n",
    "        \n",
    "        # check to see if it fucking worked\n",
    "        if np.isnan(tempDataHM).any():\n",
    "            print(\"wtf\")\n",
    "        currDataHM[:,:,k]=tempDataHM\n",
    "        \n",
    "        # do the same thing but for the LM\n",
    "        tempDataLM=currDataLM[:,:,k]\n",
    "        tempDataLM[np.abs(tempDataLM)>5000]=np.mean(tempDataLM[np.abs(tempDataLM)<5000])\n",
    "        \n",
    "        # doesnt work, will deal with the nans below\n",
    "        tempDataLM=np.nan_to_num(tempDataLM, nan=np.mean(tempDataLM[np.abs(tempDataLM)<5000]))\n",
    "        #if np.isnan(tempDataLM).any():\n",
    "        #    print(\"wtf LM\",np.mean(tempDataLM[np.abs(tempDataLM)<5000]))\n",
    "        currDataLM[:,:,k]=tempDataLM\n",
    "\n",
    "    # now build one array with everything, if its the first line make it, otherwise append\n",
    "    if line==lineNumbers[0]:\n",
    "        dataLM=currDataLM\n",
    "        dataHM=currDataHM\n",
    "        labelsLM=currLabelsLM\n",
    "        labelsHM=currLabelsHM\n",
    "    else:\n",
    "        dataLM=np.append(dataLM,currDataLM, axis=0)\n",
    "        dataHM=np.append(dataHM,currDataHM, axis=0)\n",
    "        labelsLM=np.append(labelsLM,currLabelsLM, axis=0)\n",
    "        labelsHM=np.append(labelsHM,currLabelsHM, axis=0)\n",
    "\n",
    "# drop rows where the lm and hm labels dont agree\n",
    "indexGood=labelsHM==labelsLM\n",
    "\n",
    "# see how many locations this happened in\n",
    "print(np.sum(np.abs(labelsLM-labelsHM)))\n",
    "\n",
    "# keep only the rows where they agree\n",
    "dataLM=dataLM[indexGood[:,0],:,:]\n",
    "dataHM=dataHM[indexGood[:,0],:,:]\n",
    "labelsLM=labelsLM[indexGood[:,0],:]\n",
    "labelsHM=labelsHM[indexGood[:,0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 20000\n",
      "number of test examples = 3760\n",
      "X_train shape: (20000, 3, 37, 2)\n",
      "Y_train shape: (20000, 1)\n",
      "X_test shape: (3760, 3, 37, 2)\n",
      "Y_test shape: (3760, 1)\n"
     ]
    }
   ],
   "source": [
    "# make X from the HM and LM data, X is m (examples) by 3 (first adjacent sounding, middle sounding, other adjacent sounding) \n",
    "# by 37 (timegates) by 2 (low moment then high moment)\n",
    "X=np.zeros([np.shape(dataLM)[0],np.shape(dataLM)[1],np.shape(dataLM)[2],2])\n",
    "X[:,:,:,0]=dataLM\n",
    "X[:,:,:,1]=dataHM\n",
    "\n",
    "# set nans to 0 (there shouldnt be any but who knows)\n",
    "X=np.nan_to_num(X)\n",
    "\n",
    "# scale each timegate to be between -1 and 1\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "for i in range(np.shape(X)[-1]):\n",
    "    for j in range(np.shape(X)[1]):\n",
    "        \n",
    "        # do it for all examples, for this moment\n",
    "        X[:,j,:,i] = min_max_scaler.fit_transform(X[:,j,:,i])\n",
    "        \n",
    "        # for the timegates where the LM never exists, set it back to 0 so it sits in the middle of the scaled range\n",
    "        for k in range(np.shape(X)[2]):\n",
    "            if i==0:\n",
    "                if k>=28:\n",
    "                    X[:,j,k,i]=X[:,j,k,i]-X[:,j,k,i]\n",
    "\n",
    "# make some random indices, then take 20000 examples for training and the rest for test, results in about 85/15 split\n",
    "indices=np.random.permutation(X.shape[0])\n",
    "X_train=X[indices[:20000],:,:,:]\n",
    "X_test=X[indices[20000:],:,:,:]\n",
    "Y_train=labelsLM[indices[:20000],:]\n",
    "Y_test=labelsLM[indices[20000:],:]\n",
    "\n",
    "# print out some shit\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  11.,   13.,   78.,  247.,  636., 1778., 5079., 8852., 5987.,\n",
       "        1079.]),\n",
       " array([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at a timegate to check the scaling\n",
    "plt.hist(X[:,1,3,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "\n",
    "    \"\"\"\n",
    "    input_shape: The height, width and channels as a tuple.  \n",
    "        Note that this does not include the 'batch' as a dimension.\n",
    "        If you have a batch like 'X_train', \n",
    "        then you can provide the input_shape using\n",
    "        X_train.shape[1:]\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    #X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # initializer to use\n",
    "    initToUse=keras.initializers.glorot_normal(seed=0)\n",
    "    #initToUse=keras.initializers.he_normal(seed=0)\n",
    "    \n",
    "    # for the leakly relu\n",
    "    alphaParam=0.3\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(16, (2, 3), strides = (1, 1), kernel_initializer=initToUse, name = 'conv0', padding='same')(X_input)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool0')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(32, (2, 5), strides = (1, 1), kernel_initializer=initToUse, name = 'conv1', padding='same')(X)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool1')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(64, (2, 7), strides = (1, 1), kernel_initializer=initToUse, name = 'conv2', padding='same')(X)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool2')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(128, (2, 9), strides = (1, 1), kernel_initializer=initToUse, name = 'conv3', padding='same')(X)\n",
    "    X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    X = Conv2D(128, (2, 11), strides = (1, 1), kernel_initializer=initToUse, name = 'conv4', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    \n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool_final')(X)\n",
    "    \n",
    "    # make another initializer\n",
    "    initToUse2=keras.initializers.he_normal(seed=0)\n",
    "    \n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(200, activation='relu', kernel_initializer=initToUse2,name='fc0')(X)\n",
    "    X = Dense(100, activation='relu', kernel_initializer=initToUse2,name='fc1')(X)\n",
    "    X = Dense(50, activation='relu', kernel_initializer=initToUse2,name='fc2')(X)\n",
    "    X = Dense(1, activation='sigmoid', kernel_initializer=initToUse,name='fc3')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelv2(inputShape):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # make the initializer\n",
    "    initToUse=keras.initializers.glorot_normal(seed=0)\n",
    "    initToUse2=keras.initializers.he_normal(seed=0)\n",
    "\n",
    "    # for the leakly relu\n",
    "    alphaParam=0.3\n",
    "    \n",
    "    # add some conv layers\n",
    "    model.add(Conv2D(16, (2, 3), input_shape=inputShape, kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    model.add(Conv2D(32, (2, 5), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    model.add(Conv2D(64, (2, 7), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (2, 9), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    model.add(Conv2D(128, (2, 11), kernel_initializer=initToUse, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(1,2), strides=None))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=alphaParam))\n",
    "    \n",
    "    # flatten and do some dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(160, activation='relu'))\n",
    "    model.add(Dense(96, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    #X = Conv2D(16, (2, 3), strides = (1, 1), kernel_initializer=initToUse, name = 'conv0', padding='same')(X_input)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool0')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(32, (2, 5), strides = (1, 1), kernel_initializer=initToUse, name = 'conv1', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool1')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(64, (2, 7), strides = (1, 1), kernel_initializer=initToUse, name = 'conv2', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool2')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(128, (2, 9), strides = (1, 1), kernel_initializer=initToUse, name = 'conv3', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    #X = Conv2D(128, (2, 11), strides = (1, 1), kernel_initializer=initToUse, name = 'conv4', padding='same')(X)\n",
    "    #X = MaxPooling2D((1, 2), name='max_pool3')(X)\n",
    "    #X = keras.activations.relu(X,alpha=alphaParam)\n",
    "    # input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "    # this applies 32 convolution filters of size 3x3 each.\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(256, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1208: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\dlwin36v2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1297: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/300\n",
      "20000/20000 [==============================] - 5s - loss: 0.6023 - acc: 0.7131     \n",
      "Epoch 2/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5642 - acc: 0.7328     \n",
      "Epoch 3/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5541 - acc: 0.7480     \n",
      "Epoch 4/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5423 - acc: 0.7569     \n",
      "Epoch 5/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5400 - acc: 0.7568     \n",
      "Epoch 6/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5318 - acc: 0.7594     \n",
      "Epoch 7/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5390 - acc: 0.7541     \n",
      "Epoch 8/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5240 - acc: 0.7654     \n",
      "Epoch 9/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5040 - acc: 0.7791     \n",
      "Epoch 10/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5038 - acc: 0.7809     \n",
      "Epoch 11/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.5018 - acc: 0.7775     \n",
      "Epoch 12/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4961 - acc: 0.7834     \n",
      "Epoch 13/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4907 - acc: 0.7869     \n",
      "Epoch 14/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4929 - acc: 0.7844     \n",
      "Epoch 15/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4800 - acc: 0.7955     \n",
      "Epoch 16/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4794 - acc: 0.7948     - ETA: 0s - loss: 0.4690 - acc: 0.\n",
      "Epoch 17/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4917 - acc: 0.7874     \n",
      "Epoch 18/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4745 - acc: 0.7977     \n",
      "Epoch 19/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4748 - acc: 0.7946     \n",
      "Epoch 20/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4854 - acc: 0.7892     \n",
      "Epoch 21/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4745 - acc: 0.7944     \n",
      "Epoch 22/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4684 - acc: 0.8002     \n",
      "Epoch 23/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4598 - acc: 0.8021     \n",
      "Epoch 24/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4736 - acc: 0.7933     \n",
      "Epoch 25/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4709 - acc: 0.7924     \n",
      "Epoch 26/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4551 - acc: 0.8084     \n",
      "Epoch 27/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4528 - acc: 0.8057     \n",
      "Epoch 28/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4418 - acc: 0.8129     \n",
      "Epoch 29/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4345 - acc: 0.8160     \n",
      "Epoch 30/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4270 - acc: 0.8213     \n",
      "Epoch 31/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4413 - acc: 0.8121     \n",
      "Epoch 32/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4236 - acc: 0.8212     \n",
      "Epoch 33/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4303 - acc: 0.8182     \n",
      "Epoch 34/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4524 - acc: 0.8068     \n",
      "Epoch 35/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4319 - acc: 0.8184     \n",
      "Epoch 36/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4333 - acc: 0.8173     \n",
      "Epoch 37/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4226 - acc: 0.8204     \n",
      "Epoch 38/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4102 - acc: 0.8271     \n",
      "Epoch 39/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4214 - acc: 0.8210     \n",
      "Epoch 40/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3977 - acc: 0.8319     \n",
      "Epoch 41/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4010 - acc: 0.8327     \n",
      "Epoch 42/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4073 - acc: 0.8281     \n",
      "Epoch 43/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.4091 - acc: 0.8280     \n",
      "Epoch 44/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3870 - acc: 0.8405     \n",
      "Epoch 45/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3856 - acc: 0.8375     \n",
      "Epoch 46/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3868 - acc: 0.8375     \n",
      "Epoch 47/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3871 - acc: 0.8363     \n",
      "Epoch 48/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3964 - acc: 0.8344     \n",
      "Epoch 49/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3835 - acc: 0.8399     \n",
      "Epoch 50/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3812 - acc: 0.8394     \n",
      "Epoch 51/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3821 - acc: 0.8379     \n",
      "Epoch 52/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3949 - acc: 0.8340     \n",
      "Epoch 53/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3666 - acc: 0.8481     \n",
      "Epoch 54/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3687 - acc: 0.8453     \n",
      "Epoch 55/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3799 - acc: 0.8395     \n",
      "Epoch 56/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3586 - acc: 0.8503     \n",
      "Epoch 57/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3612 - acc: 0.8492     \n",
      "Epoch 58/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3620 - acc: 0.8490     \n",
      "Epoch 59/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3637 - acc: 0.8498     \n",
      "Epoch 60/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3488 - acc: 0.8555     \n",
      "Epoch 61/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3628 - acc: 0.8495     \n",
      "Epoch 62/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3492 - acc: 0.8557     \n",
      "Epoch 63/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3496 - acc: 0.8541     \n",
      "Epoch 64/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3498 - acc: 0.8534     \n",
      "Epoch 65/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3399 - acc: 0.8589     \n",
      "Epoch 66/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3445 - acc: 0.8562     \n",
      "Epoch 67/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3473 - acc: 0.8546     \n",
      "Epoch 68/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3369 - acc: 0.8610     \n",
      "Epoch 69/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3274 - acc: 0.8646     \n",
      "Epoch 70/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3266 - acc: 0.8648     \n",
      "Epoch 71/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3422 - acc: 0.8547     - ETA: 0s - loss: 0.3371 - acc: \n",
      "Epoch 72/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3191 - acc: 0.8678     \n",
      "Epoch 73/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3146 - acc: 0.8695     \n",
      "Epoch 74/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3269 - acc: 0.8642     \n",
      "Epoch 75/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3117 - acc: 0.8712     \n",
      "Epoch 76/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3087 - acc: 0.8728     \n",
      "Epoch 77/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3173 - acc: 0.8700     \n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s - loss: 0.3086 - acc: 0.8719     \n",
      "Epoch 79/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3104 - acc: 0.8718     \n",
      "Epoch 80/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3059 - acc: 0.8734     \n",
      "Epoch 81/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2993 - acc: 0.8777     \n",
      "Epoch 82/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3125 - acc: 0.8683     \n",
      "Epoch 83/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.3019 - acc: 0.8748     \n",
      "Epoch 84/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2976 - acc: 0.8785     \n",
      "Epoch 85/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2966 - acc: 0.8786     \n",
      "Epoch 86/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2946 - acc: 0.8774     \n",
      "Epoch 87/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2930 - acc: 0.8777     \n",
      "Epoch 88/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2890 - acc: 0.8828     \n",
      "Epoch 89/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2911 - acc: 0.8795     \n",
      "Epoch 90/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2888 - acc: 0.8832     \n",
      "Epoch 91/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2897 - acc: 0.8808     \n",
      "Epoch 92/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2801 - acc: 0.8832     \n",
      "Epoch 93/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2833 - acc: 0.8852     \n",
      "Epoch 94/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2763 - acc: 0.8864     \n",
      "Epoch 95/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2799 - acc: 0.8843     \n",
      "Epoch 96/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2793 - acc: 0.8845     \n",
      "Epoch 97/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2762 - acc: 0.8875     \n",
      "Epoch 98/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2785 - acc: 0.8850     \n",
      "Epoch 99/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2735 - acc: 0.8888     \n",
      "Epoch 100/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2740 - acc: 0.8894     \n",
      "Epoch 101/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2592 - acc: 0.8937     \n",
      "Epoch 102/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2653 - acc: 0.8919     \n",
      "Epoch 103/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2756 - acc: 0.8865     \n",
      "Epoch 104/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2569 - acc: 0.8948     \n",
      "Epoch 105/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2714 - acc: 0.8879     \n",
      "Epoch 106/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2664 - acc: 0.8897     \n",
      "Epoch 107/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2672 - acc: 0.8883     \n",
      "Epoch 108/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2733 - acc: 0.8868     \n",
      "Epoch 109/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2541 - acc: 0.8963     \n",
      "Epoch 110/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2721 - acc: 0.8882     \n",
      "Epoch 111/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2505 - acc: 0.8972     \n",
      "Epoch 112/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2487 - acc: 0.8991     \n",
      "Epoch 113/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2452 - acc: 0.8989     \n",
      "Epoch 114/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2559 - acc: 0.8948     \n",
      "Epoch 115/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2401 - acc: 0.9034     \n",
      "Epoch 116/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2474 - acc: 0.8987     \n",
      "Epoch 117/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2462 - acc: 0.9003     \n",
      "Epoch 118/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2446 - acc: 0.8974     \n",
      "Epoch 119/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2687 - acc: 0.8888     \n",
      "Epoch 120/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2377 - acc: 0.9033     \n",
      "Epoch 121/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2461 - acc: 0.8991     \n",
      "Epoch 122/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2355 - acc: 0.9023     - ETA: 0s - loss: 0.2330 - acc: 0.9\n",
      "Epoch 123/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2524 - acc: 0.8978     \n",
      "Epoch 124/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2412 - acc: 0.8995     \n",
      "Epoch 125/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2385 - acc: 0.9020     \n",
      "Epoch 126/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2419 - acc: 0.9011     \n",
      "Epoch 127/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2346 - acc: 0.9036     \n",
      "Epoch 128/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2342 - acc: 0.9033     \n",
      "Epoch 129/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2380 - acc: 0.9018     \n",
      "Epoch 130/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2322 - acc: 0.9048     \n",
      "Epoch 131/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2304 - acc: 0.9062     \n",
      "Epoch 132/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2361 - acc: 0.9047     \n",
      "Epoch 133/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2278 - acc: 0.9067     \n",
      "Epoch 134/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2317 - acc: 0.9069     \n",
      "Epoch 135/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2372 - acc: 0.9034     \n",
      "Epoch 136/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2299 - acc: 0.9049     \n",
      "Epoch 137/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2363 - acc: 0.9035     \n",
      "Epoch 138/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2280 - acc: 0.9074     \n",
      "Epoch 139/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2140 - acc: 0.9110     \n",
      "Epoch 140/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2221 - acc: 0.9092     \n",
      "Epoch 141/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2251 - acc: 0.9074     \n",
      "Epoch 142/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2194 - acc: 0.9093     \n",
      "Epoch 143/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2229 - acc: 0.9068     \n",
      "Epoch 144/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2120 - acc: 0.9125     \n",
      "Epoch 145/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2152 - acc: 0.9090     \n",
      "Epoch 146/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2249 - acc: 0.9090     \n",
      "Epoch 147/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2091 - acc: 0.9138     \n",
      "Epoch 148/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2109 - acc: 0.9121     \n",
      "Epoch 149/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2182 - acc: 0.9106     \n",
      "Epoch 150/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2043 - acc: 0.9158     \n",
      "Epoch 151/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2067 - acc: 0.9146     \n",
      "Epoch 152/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2091 - acc: 0.9142     \n",
      "Epoch 153/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2062 - acc: 0.9149     \n",
      "Epoch 154/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2012 - acc: 0.9175     \n",
      "Epoch 155/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2062 - acc: 0.9121     \n",
      "Epoch 156/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2066 - acc: 0.9130     \n",
      "Epoch 157/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2104 - acc: 0.9128     \n",
      "Epoch 158/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2011 - acc: 0.9146     \n",
      "Epoch 159/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.2107 - acc: 0.9148     \n",
      "Epoch 160/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1961 - acc: 0.9191     \n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s - loss: 0.1941 - acc: 0.9186     \n",
      "Epoch 162/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1927 - acc: 0.9186     \n",
      "Epoch 163/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1884 - acc: 0.9209     \n",
      "Epoch 164/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1905 - acc: 0.9233     \n",
      "Epoch 165/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1885 - acc: 0.9219     \n",
      "Epoch 166/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1883 - acc: 0.9209     \n",
      "Epoch 167/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1866 - acc: 0.9218     \n",
      "Epoch 168/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1941 - acc: 0.9206     \n",
      "Epoch 169/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1946 - acc: 0.9193     \n",
      "Epoch 170/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1877 - acc: 0.9230     \n",
      "Epoch 171/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1841 - acc: 0.9250     \n",
      "Epoch 172/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1826 - acc: 0.9241     \n",
      "Epoch 173/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1935 - acc: 0.9208     \n",
      "Epoch 174/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1931 - acc: 0.9190     \n",
      "Epoch 175/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1706 - acc: 0.9316     \n",
      "Epoch 176/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1676 - acc: 0.9300     \n",
      "Epoch 177/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1796 - acc: 0.9249     \n",
      "Epoch 178/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1840 - acc: 0.9218     \n",
      "Epoch 179/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1787 - acc: 0.9247     \n",
      "Epoch 180/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1823 - acc: 0.9244     \n",
      "Epoch 181/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1755 - acc: 0.9270     \n",
      "Epoch 182/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1677 - acc: 0.9310     \n",
      "Epoch 183/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1639 - acc: 0.9326     \n",
      "Epoch 184/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1627 - acc: 0.9325     \n",
      "Epoch 185/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1712 - acc: 0.9293     \n",
      "Epoch 186/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1578 - acc: 0.9321     \n",
      "Epoch 187/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1608 - acc: 0.9324     \n",
      "Epoch 188/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1687 - acc: 0.9294     \n",
      "Epoch 189/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1543 - acc: 0.9341     \n",
      "Epoch 190/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1656 - acc: 0.9306     \n",
      "Epoch 191/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1590 - acc: 0.9332     \n",
      "Epoch 192/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1575 - acc: 0.9347     \n",
      "Epoch 193/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1449 - acc: 0.9380     \n",
      "Epoch 194/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1571 - acc: 0.9354     \n",
      "Epoch 195/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1490 - acc: 0.9375     \n",
      "Epoch 196/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1642 - acc: 0.9327     \n",
      "Epoch 197/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1440 - acc: 0.9400     \n",
      "Epoch 198/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1529 - acc: 0.9378     \n",
      "Epoch 199/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1527 - acc: 0.9363     \n",
      "Epoch 200/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1464 - acc: 0.9402     \n",
      "Epoch 201/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1747 - acc: 0.9281     \n",
      "Epoch 202/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1382 - acc: 0.9420     \n",
      "Epoch 203/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1409 - acc: 0.9409     \n",
      "Epoch 204/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1450 - acc: 0.9396     \n",
      "Epoch 205/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1326 - acc: 0.9432     \n",
      "Epoch 206/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1342 - acc: 0.9449     \n",
      "Epoch 207/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1449 - acc: 0.9395     \n",
      "Epoch 208/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1347 - acc: 0.9435     \n",
      "Epoch 209/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1305 - acc: 0.9466     \n",
      "Epoch 210/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1227 - acc: 0.9493     \n",
      "Epoch 211/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1436 - acc: 0.9408     \n",
      "Epoch 212/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1253 - acc: 0.9490     \n",
      "Epoch 213/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1379 - acc: 0.9429     \n",
      "Epoch 214/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1285 - acc: 0.9472     \n",
      "Epoch 215/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1201 - acc: 0.9481     \n",
      "Epoch 216/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1258 - acc: 0.9482     \n",
      "Epoch 217/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1281 - acc: 0.9468     \n",
      "Epoch 218/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1205 - acc: 0.9475     \n",
      "Epoch 219/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1180 - acc: 0.9513     \n",
      "Epoch 220/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1190 - acc: 0.9502     \n",
      "Epoch 221/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1181 - acc: 0.9504     \n",
      "Epoch 222/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1247 - acc: 0.9484     \n",
      "Epoch 223/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1252 - acc: 0.9494     \n",
      "Epoch 224/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1346 - acc: 0.9460     \n",
      "Epoch 225/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1217 - acc: 0.9493     \n",
      "Epoch 226/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1156 - acc: 0.9526     \n",
      "Epoch 227/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1113 - acc: 0.9549     \n",
      "Epoch 228/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1062 - acc: 0.9572     \n",
      "Epoch 229/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1062 - acc: 0.9558     \n",
      "Epoch 230/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1024 - acc: 0.9596     \n",
      "Epoch 231/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1209 - acc: 0.9514     \n",
      "Epoch 232/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1124 - acc: 0.9536     \n",
      "Epoch 233/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1029 - acc: 0.9590     \n",
      "Epoch 234/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0985 - acc: 0.9606     \n",
      "Epoch 235/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1018 - acc: 0.9589     \n",
      "Epoch 236/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0974 - acc: 0.9599     \n",
      "Epoch 237/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0993 - acc: 0.9590     \n",
      "Epoch 238/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1021 - acc: 0.9583     \n",
      "Epoch 239/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0983 - acc: 0.9598     \n",
      "Epoch 240/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0880 - acc: 0.9646     \n",
      "Epoch 241/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1044 - acc: 0.9596     \n",
      "Epoch 242/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0913 - acc: 0.9645     \n",
      "Epoch 243/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0930 - acc: 0.9622     \n",
      "Epoch 244/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 0s - loss: 0.0906 - acc: 0.9636     \n",
      "Epoch 245/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0879 - acc: 0.9639     \n",
      "Epoch 246/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0893 - acc: 0.9618     \n",
      "Epoch 247/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0992 - acc: 0.9605     \n",
      "Epoch 248/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0959 - acc: 0.9614     \n",
      "Epoch 249/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0939 - acc: 0.9632     \n",
      "Epoch 250/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.1029 - acc: 0.9590     \n",
      "Epoch 251/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0772 - acc: 0.9702     \n",
      "Epoch 252/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0844 - acc: 0.9653     \n",
      "Epoch 253/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0780 - acc: 0.9695     \n",
      "Epoch 254/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0799 - acc: 0.9668     \n",
      "Epoch 255/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0770 - acc: 0.9696     \n",
      "Epoch 256/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0788 - acc: 0.9691     \n",
      "Epoch 257/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0826 - acc: 0.9664     \n",
      "Epoch 258/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0880 - acc: 0.9670     \n",
      "Epoch 259/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0766 - acc: 0.9697     \n",
      "Epoch 260/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0771 - acc: 0.9694     \n",
      "Epoch 261/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0833 - acc: 0.9673     \n",
      "Epoch 262/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0689 - acc: 0.9706     \n",
      "Epoch 263/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0789 - acc: 0.9671     \n",
      "Epoch 264/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0742 - acc: 0.9727     \n",
      "Epoch 265/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0788 - acc: 0.9698     \n",
      "Epoch 266/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0770 - acc: 0.9699     \n",
      "Epoch 267/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0724 - acc: 0.9708     \n",
      "Epoch 268/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0790 - acc: 0.9691     \n",
      "Epoch 269/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0698 - acc: 0.9724     \n",
      "Epoch 270/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0711 - acc: 0.9719     \n",
      "Epoch 271/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0730 - acc: 0.9720     \n",
      "Epoch 272/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0756 - acc: 0.9712     - ETA: 0s - loss: 0.0759 - acc: 0.971\n",
      "Epoch 273/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0695 - acc: 0.9719     \n",
      "Epoch 274/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0646 - acc: 0.9748     \n",
      "Epoch 275/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0926 - acc: 0.9645     \n",
      "Epoch 276/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0618 - acc: 0.9763     \n",
      "Epoch 277/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0782 - acc: 0.9695     \n",
      "Epoch 278/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0585 - acc: 0.9785     \n",
      "Epoch 279/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0772 - acc: 0.9712     \n",
      "Epoch 280/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0740 - acc: 0.9698     \n",
      "Epoch 281/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0583 - acc: 0.9788     \n",
      "Epoch 282/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0883 - acc: 0.9674     \n",
      "Epoch 283/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0613 - acc: 0.9760     \n",
      "Epoch 284/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0649 - acc: 0.9748     \n",
      "Epoch 285/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0625 - acc: 0.9758     \n",
      "Epoch 286/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0774 - acc: 0.9713     \n",
      "Epoch 287/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0575 - acc: 0.9779     \n",
      "Epoch 288/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0639 - acc: 0.9761     \n",
      "Epoch 289/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0808 - acc: 0.9690     \n",
      "Epoch 290/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0503 - acc: 0.9815     \n",
      "Epoch 291/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0614 - acc: 0.9772     \n",
      "Epoch 292/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0563 - acc: 0.9796     \n",
      "Epoch 293/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0539 - acc: 0.9798     \n",
      "Epoch 294/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0611 - acc: 0.9762     \n",
      "Epoch 295/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0555 - acc: 0.9795     \n",
      "Epoch 296/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0540 - acc: 0.9787     \n",
      "Epoch 297/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0608 - acc: 0.9773     \n",
      "Epoch 298/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0581 - acc: 0.9791     \n",
      "Epoch 299/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0449 - acc: 0.9832     \n",
      "Epoch 300/300\n",
      "20000/20000 [==============================] - 0s - loss: 0.0600 - acc: 0.9758     \n",
      "3744/3760 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8797060686857142, 0.8521276595744681]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set epochs and everything\n",
    "learningRate=0.0008\n",
    "epochNum=300\n",
    "#epochNum=100   # using shorter for faster training\n",
    "batchSize=480  # best was 475 but 480 is a multiple of 32, fits in memory better?\n",
    "\n",
    "adam=keras.optimizers.Adam(beta_1=0.9, beta_2=0.999,lr=learningRate)\n",
    "Model1 = modelv2(X_train.shape[1:] )\n",
    "Model1.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "Model1.fit(x = X_train, y = Y_train, epochs = epochNum, batch_size = batchSize,  verbose=1)\n",
    "Model1.evaluate(x = X_test, y = Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3392/3760 [==========================>...] - ETA: 0s\n",
      " Final ensemble test accuracy of:  86.17 , mean test accuracy of: 83.63 with a std of:  0.16\n"
     ]
    }
   ],
   "source": [
    "# set epochs and everything\n",
    "learningRate=0.0008\n",
    "epochNum=250\n",
    "#epochNum=100   # using shorter for faster training\n",
    "batchSize=480  # best was 475 but 480 is a multiple of 32, fits in memory better?\n",
    "adam=keras.optimizers.Adam(beta_1=0.9, beta_2=0.999,lr=learningRate)\n",
    "test_acc=np.zeros((5,1))\n",
    "\n",
    "# make a list of models\n",
    "ensembleSize=5\n",
    "Models=[]\n",
    "\n",
    "# train the ensemble of models\n",
    "for i in range(ensembleSize):\n",
    "    \n",
    "    # make the model\n",
    "    Models.append(modelv2(X_train.shape[1:] ))\n",
    "    \n",
    "    # compile the model\n",
    "    Models[i].compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    " \n",
    "    # fit the model\n",
    "    Models[i].fit(x = X_train, y = Y_train, epochs = epochNum, batch_size = batchSize,  verbose=0)\n",
    "\n",
    "    # predict\n",
    "    temp=Models[i].evaluate(x = X_test, y = Y_test)\n",
    "    test_acc[i]=temp[1]\n",
    "\n",
    "test_acc_var=np.var(100*test_acc)\n",
    "\n",
    "# get the mean label, round it and find the emsemble test acc\n",
    "meanLabels=(Models[0].predict(X_test)+Models[1].predict(X_test)+Models[2].predict(X_test)+Models[3].predict(X_test)+Models[4].predict(X_test))/5\n",
    "meanLabels=np.round(meanLabels)\n",
    "wrongLabels=Y_test-meanLabels\n",
    "test_acc_mean=1-np.sum(np.abs(wrongLabels))/len(Y_test)\n",
    "\n",
    "print(\"\\n Final ensemble test accuracy of: \",str(np.round(test_acc_mean*100,2)),\", mean test accuracy of:\",np.str(np.round(np.mean(test_acc)*100,2)),\"with a std of: \",str(np.round(test_acc_var,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'axzhorty' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-3ea0b63ba47b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m         text = ax.text(j, i, confusionMat[i, j],\n\u001b[0;32m     25\u001b[0m                        ha=\"center\", va=\"center\", color=\"w\")\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0maxzhorty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m#x.set_title(\"Harvest of local farmers (in tons/year)\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'axzhorty' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vegetables = [\"Bad Data\",\"Good Data\"]\n",
    "farmers = [\"Bad Data\",\"Good Data\"]\n",
    "\n",
    "\n",
    "confusionMat = sklearn.metrics.confusion_matrix(Y_test,meanLabels)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(confusionMat)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(farmers)))\n",
    "ax.set_yticks(np.arange(len(vegetables)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(farmers)\n",
    "ax.set_yticklabels(vegetables)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(vegetables)):\n",
    "    for j in range(len(farmers)):\n",
    "        text = ax.text(j, i, confusionMat[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "axzhortf\n",
    "#x.set_title(\"Harvest of local farmers (in tons/year)\")\n",
    "fig.tight_layout()\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper paramater tuning, currently just set up for learning rate, batch size and # of epochs\n",
    "\n",
    "# values already tried, Feb 19th\n",
    "#lr_array=np.array([0.1,0.05,0.01,0.005,0.001])\n",
    "#batch_size_array=np.array([16,32,64,128,256,512,1024])\n",
    "#epoch_array=np.array([10,20,40,80,160])\n",
    "\n",
    "# set the arrays Feb 19th, best train acc was at 0.0008 200 450, best test was at 0.0008 160 450\n",
    "#lr_array=np.array([0.005,0.003,0.001,0.0008,0.0005,0.0001])\n",
    "#batch_size_array=np.array([400,450,500,512,550])\n",
    "#epoch_array=np.array([100,130,160,180,200])\n",
    "\n",
    "# set the arrays, best test acc was 85%, train acc of 97% at 0.0008 250 475 (got to 100% train acc with longer train time, 500 epochs)\n",
    "lr_array=np.array([0.0009,0.0008,0.0007,0.0006])\n",
    "batch_size_array=np.array([425,450,475])\n",
    "epoch_array=np.array([160,170,250,500])\n",
    "\n",
    "\n",
    "# get their sizes and their final size\n",
    "lr_array_size=np.shape(lr_array)[0]\n",
    "batch_size_array_size=np.shape(batch_size_array)[0]\n",
    "epoch_array_size=np.shape(epoch_array)[0]\n",
    "final_size=lr_array_size*batch_size_array_size*epoch_array_size\n",
    "\n",
    "# tile and reshape them\n",
    "lr_array=np.tile(lr_array,final_size).squeeze()\n",
    "batch_size_array=np.reshape(np.tile(np.tile(batch_size_array,(lr_array_size,1)).T,(epoch_array_size,1)),[final_size,1]).squeeze()\n",
    "epoch_array=np.reshape(np.tile(epoch_array,(lr_array_size*batch_size_array_size,1)).T,[1,final_size]).squeeze()\n",
    "\n",
    "# make lists and arrays for the results\n",
    "training_history=[]\n",
    "training_history_acc=np.zeros((final_size,1))\n",
    "preds_history=[]\n",
    "preds_loss=np.zeros((final_size,1))\n",
    "preds_acc=np.zeros((final_size,1))\n",
    "\n",
    "# loop de loop\n",
    "for i in range(final_size):\n",
    "    \n",
    "    # make the model and its opimizer\n",
    "    happyModel = model(X_train.shape[1:] )\n",
    "    adam=keras.optimizers.Adam(beta_1=0.9, beta_2=0.999,lr=lr_array[i])\n",
    "    \n",
    "    # compile the model\n",
    "    happyModel.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    # fit the model\n",
    "    temp=happyModel.fit(x = X_train, y = Y_train, epochs = np.int(epoch_array[i]), batch_size = np.int(batch_size_array[i]),  verbose=0)\n",
    "    \n",
    "    # store how it did\n",
    "    training_history_acc[i]=temp.history['acc'][-1]\n",
    "    training_history.append(temp)\n",
    "    \n",
    "    # predict and store on x_test\n",
    "    temp=happyModel.evaluate(x = X_test, y = Y_test)\n",
    "    preds_loss[i]=temp[0]\n",
    "    preds_acc[i]=temp[1]\n",
    "    preds_history.append(temp)\n",
    "    \n",
    "    # print a msg\n",
    "    print(\" Training accuracy = \"+str(np.round(training_history_acc[i]*100,2))+\"%, learning rate, epoch size, and mini-batch size: \"+str(lr_array[i])+\",\"+str(epoch_array[i])+\",\"+str(batch_size_array[i])+\", test loss and accuracy: \"+str(preds_loss[i])+str(np.round(preds_acc[i]*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.456235956638418, 0.7204787234042553]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current best is lr=0.0008, epochs=250, batch size=475, gives 97.4% train and 85.5% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8550531914893617 0.0008 250 475\n"
     ]
    }
   ],
   "source": [
    "best=np.argmax(preds_acc)\n",
    "print(np.max(preds_acc),lr_array[best],epoch_array[best],batch_size_array[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0009 500 425\n"
     ]
    }
   ],
   "source": [
    "best=np.argmax(training_history_acc)\n",
    "print(np.max(training_history_acc),lr_array[best],epoch_array[best],batch_size_array[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680/3760 [============================>.] - ETA: 0s\n",
      "Loss = 0.4233942541670292\n",
      "Test Accuracy = 0.8236702127659574\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds = happyModel.evaluate(x = X_test, y = Y_test)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 3, 37, 16)         208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 18, 16)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 3, 18, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 18, 32)         5152      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 3, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 9, 64)          28736     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 3, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 3, 2, 128)         360576    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 3, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 3, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 160)               41120     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 96)                15456     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                3104      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 700,529\n",
      "Trainable params: 700,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1724pt\" viewBox=\"0.00 0.00 223.00 1724.00\" width=\"223pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1720)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1720 219,-1720 219,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2334665241040 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2334665241040</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-1679.5 19.5,-1715.5 195.5,-1715.5 195.5,-1679.5 19.5,-1679.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1693.8\">conv2d_6_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 2334665240872 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2334665240872</title>\n",
       "<polygon fill=\"none\" points=\"44,-1606.5 44,-1642.5 171,-1642.5 171,-1606.5 44,-1606.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1620.8\">conv2d_6: Conv2D</text>\n",
       "</g>\n",
       "<!-- 2334665241040&#45;&gt;2334665240872 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2334665241040-&gt;2334665240872</title>\n",
       "<path d=\"M107.5,-1679.31C107.5,-1671.29 107.5,-1661.55 107.5,-1652.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1652.53 107.5,-1642.53 104,-1652.53 111,-1652.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334665243672 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2334665243672</title>\n",
       "<polygon fill=\"none\" points=\"3,-1533.5 3,-1569.5 212,-1569.5 212,-1533.5 3,-1533.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1547.8\">max_pooling2d_6: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 2334665240872&#45;&gt;2334665243672 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2334665240872-&gt;2334665243672</title>\n",
       "<path d=\"M107.5,-1606.31C107.5,-1598.29 107.5,-1588.55 107.5,-1579.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1579.53 107.5,-1569.53 104,-1579.53 111,-1579.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334665301576 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2334665301576</title>\n",
       "<polygon fill=\"none\" points=\"23,-1460.5 23,-1496.5 192,-1496.5 192,-1460.5 23,-1460.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1474.8\">leaky_re_lu_6: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 2334665243672&#45;&gt;2334665301576 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2334665243672-&gt;2334665301576</title>\n",
       "<path d=\"M107.5,-1533.31C107.5,-1525.29 107.5,-1515.55 107.5,-1506.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1506.53 107.5,-1496.53 104,-1506.53 111,-1506.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893616152 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2334893616152</title>\n",
       "<polygon fill=\"none\" points=\"44,-1387.5 44,-1423.5 171,-1423.5 171,-1387.5 44,-1387.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1401.8\">conv2d_7: Conv2D</text>\n",
       "</g>\n",
       "<!-- 2334665301576&#45;&gt;2334893616152 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2334665301576-&gt;2334893616152</title>\n",
       "<path d=\"M107.5,-1460.31C107.5,-1452.29 107.5,-1442.55 107.5,-1433.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1433.53 107.5,-1423.53 104,-1433.53 111,-1433.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334665301128 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2334665301128</title>\n",
       "<polygon fill=\"none\" points=\"3,-1314.5 3,-1350.5 212,-1350.5 212,-1314.5 3,-1314.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1328.8\">max_pooling2d_7: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 2334893616152&#45;&gt;2334665301128 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2334893616152-&gt;2334665301128</title>\n",
       "<path d=\"M107.5,-1387.31C107.5,-1379.29 107.5,-1369.55 107.5,-1360.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1360.53 107.5,-1350.53 104,-1360.53 111,-1360.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893692184 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2334893692184</title>\n",
       "<polygon fill=\"none\" points=\"23,-1241.5 23,-1277.5 192,-1277.5 192,-1241.5 23,-1241.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1255.8\">leaky_re_lu_7: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 2334665301128&#45;&gt;2334893692184 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2334665301128-&gt;2334893692184</title>\n",
       "<path d=\"M107.5,-1314.31C107.5,-1306.29 107.5,-1296.55 107.5,-1287.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1287.53 107.5,-1277.53 104,-1287.53 111,-1287.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893788184 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2334893788184</title>\n",
       "<polygon fill=\"none\" points=\"44,-1168.5 44,-1204.5 171,-1204.5 171,-1168.5 44,-1168.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1182.8\">conv2d_8: Conv2D</text>\n",
       "</g>\n",
       "<!-- 2334893692184&#45;&gt;2334893788184 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2334893692184-&gt;2334893788184</title>\n",
       "<path d=\"M107.5,-1241.31C107.5,-1233.29 107.5,-1223.55 107.5,-1214.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1214.53 107.5,-1204.53 104,-1214.53 111,-1214.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893788128 -->\n",
       "<g class=\"node\" id=\"node9\"><title>2334893788128</title>\n",
       "<polygon fill=\"none\" points=\"3,-1095.5 3,-1131.5 212,-1131.5 212,-1095.5 3,-1095.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1109.8\">max_pooling2d_8: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 2334893788184&#45;&gt;2334893788128 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2334893788184-&gt;2334893788128</title>\n",
       "<path d=\"M107.5,-1168.31C107.5,-1160.29 107.5,-1150.55 107.5,-1141.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1141.53 107.5,-1131.53 104,-1141.53 111,-1141.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893961736 -->\n",
       "<g class=\"node\" id=\"node10\"><title>2334893961736</title>\n",
       "<polygon fill=\"none\" points=\"23,-1022.5 23,-1058.5 192,-1058.5 192,-1022.5 23,-1022.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-1036.8\">leaky_re_lu_8: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 2334893788128&#45;&gt;2334893961736 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>2334893788128-&gt;2334893961736</title>\n",
       "<path d=\"M107.5,-1095.31C107.5,-1087.29 107.5,-1077.55 107.5,-1068.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-1068.53 107.5,-1058.53 104,-1068.53 111,-1068.53\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893964144 -->\n",
       "<g class=\"node\" id=\"node11\"><title>2334893964144</title>\n",
       "<polygon fill=\"none\" points=\"42.5,-949.5 42.5,-985.5 172.5,-985.5 172.5,-949.5 42.5,-949.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-963.8\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 2334893961736&#45;&gt;2334893964144 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>2334893961736-&gt;2334893964144</title>\n",
       "<path d=\"M107.5,-1022.31C107.5,-1014.29 107.5,-1004.55 107.5,-995.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-995.529 107.5,-985.529 104,-995.529 111,-995.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894054032 -->\n",
       "<g class=\"node\" id=\"node12\"><title>2334894054032</title>\n",
       "<polygon fill=\"none\" points=\"44,-876.5 44,-912.5 171,-912.5 171,-876.5 44,-876.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-890.8\">conv2d_9: Conv2D</text>\n",
       "</g>\n",
       "<!-- 2334893964144&#45;&gt;2334894054032 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>2334893964144-&gt;2334894054032</title>\n",
       "<path d=\"M107.5,-949.313C107.5,-941.289 107.5,-931.547 107.5,-922.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-922.529 107.5,-912.529 104,-922.529 111,-922.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334893964032 -->\n",
       "<g class=\"node\" id=\"node13\"><title>2334893964032</title>\n",
       "<polygon fill=\"none\" points=\"3,-803.5 3,-839.5 212,-839.5 212,-803.5 3,-803.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-817.8\">max_pooling2d_9: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 2334894054032&#45;&gt;2334893964032 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>2334894054032-&gt;2334893964032</title>\n",
       "<path d=\"M107.5,-876.313C107.5,-868.289 107.5,-858.547 107.5,-849.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-849.529 107.5,-839.529 104,-849.529 111,-849.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894154640 -->\n",
       "<g class=\"node\" id=\"node14\"><title>2334894154640</title>\n",
       "<polygon fill=\"none\" points=\"23,-730.5 23,-766.5 192,-766.5 192,-730.5 23,-730.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-744.8\">leaky_re_lu_9: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 2334893964032&#45;&gt;2334894154640 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>2334893964032-&gt;2334894154640</title>\n",
       "<path d=\"M107.5,-803.313C107.5,-795.289 107.5,-785.547 107.5,-776.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-776.529 107.5,-766.529 104,-776.529 111,-776.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894443432 -->\n",
       "<g class=\"node\" id=\"node15\"><title>2334894443432</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-657.5 40.5,-693.5 174.5,-693.5 174.5,-657.5 40.5,-657.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-671.8\">conv2d_10: Conv2D</text>\n",
       "</g>\n",
       "<!-- 2334894154640&#45;&gt;2334894443432 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>2334894154640-&gt;2334894443432</title>\n",
       "<path d=\"M107.5,-730.313C107.5,-722.289 107.5,-712.547 107.5,-703.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-703.529 107.5,-693.529 104,-703.529 111,-703.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894443376 -->\n",
       "<g class=\"node\" id=\"node16\"><title>2334894443376</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 215,-620.5 215,-584.5 0,-584.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-598.8\">max_pooling2d_10: MaxPooling2D</text>\n",
       "</g>\n",
       "<!-- 2334894443432&#45;&gt;2334894443376 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>2334894443432-&gt;2334894443376</title>\n",
       "<path d=\"M107.5,-657.313C107.5,-649.289 107.5,-639.547 107.5,-630.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-630.529 107.5,-620.529 104,-630.529 111,-630.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894633368 -->\n",
       "<g class=\"node\" id=\"node17\"><title>2334894633368</title>\n",
       "<polygon fill=\"none\" points=\"19.5,-511.5 19.5,-547.5 195.5,-547.5 195.5,-511.5 19.5,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-525.8\">leaky_re_lu_10: LeakyReLU</text>\n",
       "</g>\n",
       "<!-- 2334894443376&#45;&gt;2334894633368 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>2334894443376-&gt;2334894633368</title>\n",
       "<path d=\"M107.5,-584.313C107.5,-576.289 107.5,-566.547 107.5,-557.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-557.529 107.5,-547.529 104,-557.529 111,-557.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894636896 -->\n",
       "<g class=\"node\" id=\"node18\"><title>2334894636896</title>\n",
       "<polygon fill=\"none\" points=\"53,-438.5 53,-474.5 162,-474.5 162,-438.5 53,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-452.8\">flatten_2: Flatten</text>\n",
       "</g>\n",
       "<!-- 2334894633368&#45;&gt;2334894636896 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>2334894633368-&gt;2334894636896</title>\n",
       "<path d=\"M107.5,-511.313C107.5,-503.289 107.5,-493.547 107.5,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-484.529 107.5,-474.529 104,-484.529 111,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894826384 -->\n",
       "<g class=\"node\" id=\"node19\"><title>2334894826384</title>\n",
       "<polygon fill=\"none\" points=\"55.5,-365.5 55.5,-401.5 159.5,-401.5 159.5,-365.5 55.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-379.8\">dense_6: Dense</text>\n",
       "</g>\n",
       "<!-- 2334894636896&#45;&gt;2334894826384 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>2334894636896-&gt;2334894826384</title>\n",
       "<path d=\"M107.5,-438.313C107.5,-430.289 107.5,-420.547 107.5,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-411.529 107.5,-401.529 104,-411.529 111,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334894908584 -->\n",
       "<g class=\"node\" id=\"node20\"><title>2334894908584</title>\n",
       "<polygon fill=\"none\" points=\"55.5,-292.5 55.5,-328.5 159.5,-328.5 159.5,-292.5 55.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-306.8\">dense_7: Dense</text>\n",
       "</g>\n",
       "<!-- 2334894826384&#45;&gt;2334894908584 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>2334894826384-&gt;2334894908584</title>\n",
       "<path d=\"M107.5,-365.313C107.5,-357.289 107.5,-347.547 107.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-338.529 107.5,-328.529 104,-338.529 111,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334895030568 -->\n",
       "<g class=\"node\" id=\"node21\"><title>2334895030568</title>\n",
       "<polygon fill=\"none\" points=\"55.5,-219.5 55.5,-255.5 159.5,-255.5 159.5,-219.5 55.5,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-233.8\">dense_8: Dense</text>\n",
       "</g>\n",
       "<!-- 2334894908584&#45;&gt;2334895030568 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>2334894908584-&gt;2334895030568</title>\n",
       "<path d=\"M107.5,-292.313C107.5,-284.289 107.5,-274.547 107.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-265.529 107.5,-255.529 104,-265.529 111,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334895126512 -->\n",
       "<g class=\"node\" id=\"node22\"><title>2334895126512</title>\n",
       "<polygon fill=\"none\" points=\"55.5,-146.5 55.5,-182.5 159.5,-182.5 159.5,-146.5 55.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-160.8\">dense_9: Dense</text>\n",
       "</g>\n",
       "<!-- 2334895030568&#45;&gt;2334895126512 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>2334895030568-&gt;2334895126512</title>\n",
       "<path d=\"M107.5,-219.313C107.5,-211.289 107.5,-201.547 107.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-192.529 107.5,-182.529 104,-192.529 111,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334895350952 -->\n",
       "<g class=\"node\" id=\"node23\"><title>2334895350952</title>\n",
       "<polygon fill=\"none\" points=\"42.5,-73.5 42.5,-109.5 172.5,-109.5 172.5,-73.5 42.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-87.8\">dropout_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 2334895126512&#45;&gt;2334895350952 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>2334895126512-&gt;2334895350952</title>\n",
       "<path d=\"M107.5,-146.313C107.5,-138.289 107.5,-128.547 107.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-119.529 107.5,-109.529 104,-119.529 111,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2334895350784 -->\n",
       "<g class=\"node\" id=\"node24\"><title>2334895350784</title>\n",
       "<polygon fill=\"none\" points=\"52,-0.5 52,-36.5 163,-36.5 163,-0.5 52,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-14.8\">dense_10: Dense</text>\n",
       "</g>\n",
       "<!-- 2334895350952&#45;&gt;2334895350784 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>2334895350952-&gt;2334895350784</title>\n",
       "<path d=\"M107.5,-73.3129C107.5,-65.2895 107.5,-55.5475 107.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111,-46.5288 107.5,-36.5288 104,-46.5289 111,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(Models[0], to_file='Model_graph.png')\n",
    "SVG(model_to_dot(Models[0]).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
